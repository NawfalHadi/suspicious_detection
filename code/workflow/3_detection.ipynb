{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ceaa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "FIXED_SIZE = 640\n",
    "SMALL_SIZE = 150, 150\n",
    "NUM_FRAMES = 15\n",
    "model_yolo = YOLO(\"../../model/yolo/yolo12n.pt\") \n",
    "\n",
    "with open('../../model/trained/model_v0.pkl', 'rb') as f:\n",
    "    model_pred = pickle.load(f)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7c45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropped_frame(frame):\n",
    "    results = model_yolo(frame, classes=[0], verbose=False)\n",
    "    boxes = results[0].boxes\n",
    "    plotted_frame = results[0].plot() \n",
    "\n",
    "    try:\n",
    "        if len(boxes) > 0:\n",
    "            # Get the bounding box coordinates for the first detected object\n",
    "            x1, y1, x2, y2 = boxes.xyxy[0].cpu().numpy().astype(int)\n",
    "\n",
    "            cropped_frame = frame[y1:y2, x1:x2]\n",
    "            crop_h, crop_w = cropped_frame.shape[:2]\n",
    "\n",
    "            # We want to fit the largest dimension (width or height) to the FIXED_SIZE\n",
    "            scale = FIXED_SIZE / max(crop_w, crop_h)\n",
    "            new_w = int(crop_w * scale)\n",
    "            new_h = int(crop_h * scale)\n",
    "\n",
    "            # Resize the cropped frame to the new dimensions\n",
    "            resized_img = cv2.resize(cropped_frame, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Background frame, that are not filled with boxes\n",
    "            final_frame = np.full((FIXED_SIZE, FIXED_SIZE, 3), 255, dtype=np.uint8)\n",
    "            \n",
    "            # dw and dh are the space left over after placing the image\n",
    "            dw = FIXED_SIZE - new_w\n",
    "            dh = FIXED_SIZE - new_h\n",
    "\n",
    "            # Calculate the starting position (top-left corner) for centering\n",
    "            top = dh // 2\n",
    "            bottom = top + new_h\n",
    "            left = dw // 2\n",
    "            right = left + new_w\n",
    "\n",
    "            final_frame[top:bottom, left:right] = resized_img\n",
    "\n",
    "            return final_frame\n",
    "\n",
    "        else:\n",
    "            print(\"No objects detected in the image.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c620eac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761357239.618545     839 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1761357239.718579    4112 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: llvmpipe (LLVM 20.1.2, 256 bits)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1761357240.311038    4074 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761357240.685127    4077 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761357240.721343    4072 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761357240.722035    4073 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761357240.729832    4077 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761357240.756675    4076 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761357240.774151    4080 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761357240.775903    4075 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761357243.744391    4082 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error displaying frame: X has 20 features, but StandardScaler is expecting 300 features as input.\n",
      "Error displaying frame: X has 40 features, but StandardScaler is expecting 300 features as input.\n",
      "Error displaying frame: X has 60 features, but StandardScaler is expecting 300 features as input.\n",
      "Error displaying frame: X has 80 features, but StandardScaler is expecting 300 features as input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error displaying frame: X has 100 features, but StandardScaler is expecting 300 features as input.\n",
      "Error displaying frame: X has 120 features, but StandardScaler is expecting 300 features as input.\n",
      "Error displaying frame: X has 140 features, but StandardScaler is expecting 300 features as input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error displaying frame: X has 160 features, but StandardScaler is expecting 300 features as input.\n",
      "Error displaying frame: X has 180 features, but StandardScaler is expecting 300 features as input.\n",
      "Error displaying frame: X has 200 features, but StandardScaler is expecting 300 features as input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error displaying frame: X has 220 features, but StandardScaler is expecting 300 features as input.\n",
      "Error displaying frame: X has 240 features, but StandardScaler is expecting 300 features as input.\n",
      "Error displaying frame: X has 260 features, but StandardScaler is expecting 300 features as input.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error displaying frame: X has 280 features, but StandardScaler is expecting 300 features as input.\n"
     ]
    }
   ],
   "source": [
    "# cap = cv2.VideoCapture('http://192.168.100.197:5000/video')\n",
    "cap = cv2.VideoCapture(f'../../assets/test/test_celinguk_1.mp4')\n",
    "\n",
    "frames = []\n",
    "detected = []\n",
    "\n",
    "undetectable_image = None\n",
    "final_layout = None \n",
    "\n",
    "saved = 0\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "else:\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            start_time = time.time()\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_cropped = cropped_frame(frame)\n",
    "                \n",
    "                # FIX 1: Skip if initial cropping failed (frame_cropped is None)\n",
    "                if frame_cropped is None:\n",
    "                    print(\"Skipping frame: cropped_frame returned None.\")\n",
    "                    continue\n",
    "\n",
    "                # Prepare the current frame for side display (resized) and an empty list for detection\n",
    "                current_small_frame = cv2.resize(frame_cropped, (SMALL_SIZE))\n",
    "                current_detection_list = []\n",
    "                detection_successful = False\n",
    "\n",
    "                try:\n",
    "                    # Attempt Mediapipe detection\n",
    "                    frames_mp = cv2.cvtColor(frame_cropped, cv2.COLOR_BGR2RGB)\n",
    "                    frames_mp.flags.writeable = False\n",
    "                    \n",
    "                    results = holistic.process(frames_mp)\n",
    "                    frames_mp.flags.writeable = True\n",
    "                    frames_mp = cv2.cvtColor(frame_cropped, cv2.COLOR_RGB2BGR) \n",
    "\n",
    "                    # Extract landmarks if available\n",
    "                    if results.pose_landmarks:\n",
    "                        current_detection_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE])\n",
    "                        current_detection_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EYE])\n",
    "                        current_detection_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EYE])\n",
    "                        current_detection_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST])\n",
    "                        current_detection_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_WRIST])\n",
    "                        detection_successful = True\n",
    "                    else:\n",
    "                        raise ValueError(\"No pose landmarks detected.\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    # Detection failed (Mediapipe error or no landmarks found)\n",
    "                    error_text = f\"No detection: {e}\"\n",
    "                    cv2.putText(frame_cropped, error_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    \n",
    "                if detection_successful:\n",
    "                    # This block is now ONLY executed if detection_successful is True\n",
    "                    if len(frames) < NUM_FRAMES:\n",
    "                        frames.append(current_small_frame)\n",
    "                        detected.append(current_detection_list)\n",
    "                    else:\n",
    "                        frames.pop(0)\n",
    "                        detected.pop(0)\n",
    "                        frames.append(current_small_frame)\n",
    "                        detected.append(current_detection_list)\n",
    "                else:\n",
    "                    print(f\"Skipping frame {time.time() - start_time}ms: No pose landmarks detected.\")\n",
    "                    pass\n",
    "                \n",
    "                # --- LAYOUT AND DISPLAY LOGIC ---\n",
    "                if len(frames) == NUM_FRAMES:\n",
    "                    stacked1_frames = np.vstack((frames[0], frames[1], frames[2]))\n",
    "                    stacked2_frames = np.vstack((frames[3], frames[4], frames[5]))\n",
    "                    stacked3_frames = np.vstack((frames[6], frames[7], frames[8]))\n",
    "                    stacked4_frames = np.vstack((frames[9], frames[10], frames[11]))\n",
    "                    stacked5_frames = np.vstack((frames[12], frames[13], frames[14]))\n",
    "\n",
    "                    stacked1_frames = cv2.resize(stacked1_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked2_frames = cv2.resize(stacked2_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked3_frames = cv2.resize(stacked3_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked4_frames = cv2.resize(stacked4_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked5_frames = cv2.resize(stacked5_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "\n",
    "                    # All components are guaranteed to be valid images here\n",
    "                    final_layout = np.hstack((frame_cropped, stacked1_frames, stacked2_frames, stacked3_frames, stacked4_frames, stacked5_frames))\n",
    "                else:\n",
    "                    final_layout = frame_cropped\n",
    "                \n",
    "                if final_layout is not None and final_layout.size > 0:\n",
    "                    flat_detected = [landmark for sublist in detected for landmark in sublist]\n",
    "                    # print(len(flat_detected)) # Should always be 5 * NUM_FRAMES when full\n",
    "\n",
    "                    if len(flat_detected) > 0:\n",
    "                        landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "                        landmark_list.landmark.extend(flat_detected)\n",
    "                    \n",
    "                    try:\n",
    "                        motion_row = []\n",
    "                        \n",
    "                        for lv in landmark_list.landmark:\n",
    "                            motion_row.extend([lv.x, lv.y, lv.z, lv.visibility])\n",
    "\n",
    "                        motion_row = list(np.array(motion_row))\n",
    "                        motion_detected = pd.DataFrame([motion_row])\n",
    "                        motion_class = model_pred.predict(motion_detected)[0]\n",
    "\n",
    "                        warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "                        cv2.putText(final_layout, f'Class: {motion_class}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "                        cv2.imshow('Video', final_layout)\n",
    "\n",
    "                        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "                        if key == ord('q'):\n",
    "                            break\n",
    "                    \n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error displaying frame: {e}\")\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08444e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
