{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ceaa3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 11:39:40.414449: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "FIXED_SIZE = 640\n",
    "SMALL_SIZE = 150, 150\n",
    "NUM_FRAMES = 15\n",
    "model_yolo = YOLO(\"../../model/yolo/yolo12n.pt\") \n",
    "\n",
    "with open('../../model/trained/svc/svc_model_v2.pkl', 'rb') as f:\n",
    "    model_pred = pickle.load(f)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7c45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropped_frame(frame):\n",
    "    results = model_yolo(frame, classes=[0], verbose=False)\n",
    "    boxes = results[0].boxes\n",
    "    plotted_frame = results[0].plot() \n",
    "\n",
    "    try:\n",
    "        if len(boxes) > 0:\n",
    "            # Get the bounding box coordinates for the first detected object\n",
    "            x1, y1, x2, y2 = boxes.xyxy[0].cpu().numpy().astype(int)\n",
    "\n",
    "            cropped_frame = frame[y1:y2, x1:x2]\n",
    "            crop_h, crop_w = cropped_frame.shape[:2]\n",
    "\n",
    "            # We want to fit the largest dimension (width or height) to the FIXED_SIZE\n",
    "            scale = FIXED_SIZE / max(crop_w, crop_h)\n",
    "            new_w = int(crop_w * scale)\n",
    "            new_h = int(crop_h * scale)\n",
    "\n",
    "            # Resize the cropped frame to the new dimensions\n",
    "            resized_img = cv2.resize(cropped_frame, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Background frame, that are not filled with boxes\n",
    "            final_frame = np.full((FIXED_SIZE, FIXED_SIZE, 3), 255, dtype=np.uint8)\n",
    "            \n",
    "            # dw and dh are the space left over after placing the image\n",
    "            dw = FIXED_SIZE - new_w\n",
    "            dh = FIXED_SIZE - new_h\n",
    "\n",
    "            # Calculate the starting position (top-left corner) for centering\n",
    "            top = dh // 2\n",
    "            bottom = top + new_h\n",
    "            left = dw // 2\n",
    "            right = left + new_w\n",
    "\n",
    "            final_frame[top:bottom, left:right] = resized_img\n",
    "\n",
    "            return final_frame\n",
    "\n",
    "        else:\n",
    "            print(\"No objects detected in the image.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6acb6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_state(state, lndmrkX):\n",
    "    if state == 0:\n",
    "        if lndmrkX[\"nose\"] == min(lndmrkX.values()):\n",
    "            direction = 1 # Kiri\n",
    "        elif lndmrkX[\"nose\"] == max(lndmrkX.values()):      \n",
    "            direction = 2 # Kanan\n",
    "        else:\n",
    "            direction = 0 # Tengah\n",
    "    else:\n",
    "        if lndmrkX[\"nose\"] == min(lndmrkX.values()):\n",
    "            direction = 2 # Kanan\n",
    "        elif lndmrkX[\"nose\"] == max(lndmrkX.values()):      \n",
    "            direction = 1 # Kiri\n",
    "        else:\n",
    "            direction = 0 # Tengah\n",
    "    \n",
    "    return direction\n",
    "\n",
    "def hand_state(state, lndmrkZ):\n",
    "    if state == 0:\n",
    "        if lndmrkZ[\"wrist_r\"] and lndmrkZ[\"wrist_l\"] < lndmrkZ[\"nose\"]:\n",
    "            hand = 1 # Terlihat\n",
    "        else:\n",
    "            hand = 0 # Tidak Terlihat\n",
    "    else:\n",
    "        if lndmrkZ[\"wrist_r\"] and lndmrkZ[\"wrist_l\"] < lndmrkZ[\"nose\"]:\n",
    "            hand = 0 # Terlihat\n",
    "        else:\n",
    "            hand = 1 # Tidak Terlihat\n",
    "    return hand\n",
    "\n",
    "def get_extFeature_value(lndmrk):\n",
    "    noseX, noseY, noseZ = lndmrk[\"nose\"].x, lndmrk[\"nose\"].y, lndmrk[\"nose\"].z\n",
    "    earLX, earLY, earLZ = lndmrk[\"ear_l\"].x, lndmrk[\"ear_l\"].y, lndmrk[\"ear_l\"].z\n",
    "    earRX, earRY, earRZ = lndmrk[\"ear_r\"].x, lndmrk[\"ear_r\"].y, lndmrk[\"ear_r\"].z\n",
    "    wristRX, wristRY, wristRZ = lndmrk[\"wrist_r\"].x, lndmrk[\"wrist_r\"].y, lndmrk[\"wrist_r\"].z\n",
    "    wristLX, wristLY, wristLZ = lndmrk[\"wrist_l\"].x, lndmrk[\"wrist_l\"].y, lndmrk[\"wrist_l\"].z\n",
    "    \n",
    "    lndmrkX = {\"nose\": noseX, \"ear_l\": earLX, \"ear_r\": earRX, \"wrist_r\": wristRX, \"wrist_l\": wristLX }\n",
    "    lndmrkY = {\"nose\": noseY, \"ear_l\": earLY, \"ear_r\": earRY, \"wrist_r\": wristRY, \"wrist_l\": wristLY }\n",
    "    lndmrkZ = {\"nose\": noseZ, \"ear_l\": earLZ, \"ear_r\": earRZ, \"wrist_r\": wristRZ, \"wrist_l\": wristLZ }\n",
    "\n",
    "    if noseZ < (earLZ and earRZ):\n",
    "        state = 0\n",
    "        side = side_state(0, lndmrkX)\n",
    "        hand = hand_state(0, lndmrkZ)\n",
    "    else:\n",
    "        state = 1\n",
    "        side = side_state(1, lndmrkX)\n",
    "        hand = hand_state(1, lndmrkZ)\n",
    "\n",
    "    return side, state, hand\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c44fc83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classficiation_missing_calculated(class_name, percentage, counter, global_class):\n",
    "    # The new, calculated classification, default to keeping the old one\n",
    "    new_classification = global_class\n",
    "    new_counter = counter\n",
    "    \n",
    "    # Check if the current frame meets the confidence threshold (>= 75.0%)\n",
    "    if percentage >= 75.0:\n",
    "        # 1. The confidence is high enough. We increment the counter.\n",
    "        new_counter += 1\n",
    "        \n",
    "        # 2. Check if the counter has reached the persistence threshold (>= 10)\n",
    "        if new_counter >= 10:\n",
    "            # We have 10 consecutive high-confidence frames!\n",
    "            new_classification = class_name # Update the classification\n",
    "            new_counter = 0 # Reset the counter to start over\n",
    "            print(f\"Updated Classification to: {new_classification} with confidence {percentage:.2f}%\")\n",
    "    else:\n",
    "        # The confidence is too low (< 75.0%). Reset the counter regardless of the class.\n",
    "        new_counter = 0\n",
    "        \n",
    "    return new_classification, percentage, new_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6b5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common parameters for clarity\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 1\n",
    "TEXT_THICKNESS = 2\n",
    "TEXT_COLOR = (255, 255, 255) # White text\n",
    "BLUE_COLOR = (255, 0, 0)   # Blue\n",
    "RED_COLOR = (0, 0, 255) # Red\n",
    "ORG = (10, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d66102e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text_with_background(img, text, org, font, font_scale, text_color, bg_color, thickness):\n",
    "    # 1. Get the size of the text\n",
    "    (text_w, text_h), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "\n",
    "    # Calculate coordinates for the background rectangle\n",
    "    # org is the bottom-left corner of the text.\n",
    "    # We use a small padding (e.g., 5 pixels)\n",
    "    padding = 5\n",
    "    \n",
    "    # Top-left corner of the rectangle\n",
    "    x1 = org[0] - padding\n",
    "    y1 = org[1] - text_h - baseline - padding\n",
    "    \n",
    "    # Bottom-right corner of the rectangle\n",
    "    x2 = org[0] + text_w + padding\n",
    "    y2 = org[1] + baseline + padding\n",
    "\n",
    "    # 2. Draw the filled rectangle (Background)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), bg_color, -1) # -1 means filled\n",
    "\n",
    "    # 3. Draw the text over the rectangle\n",
    "    cv2.putText(img, text, org, font, font_scale, text_color, thickness)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620eac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763623551.386549     875 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1763623551.398962   62808 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: llvmpipe (LLVM 20.1.2, 256 bits)\n",
      "W0000 00:00:1763623551.724137   62796 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763623551.909395   62800 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763623551.926335   62802 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763623551.927171   62798 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763623551.937730   62807 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763623551.960024   62800 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763623551.960538   62796 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763623551.966320   62797 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping frame 0.5694031715393066ms: No pose landmarks detected.\n",
      "Error displaying frame: list index out of range\n",
      "Error displaying frame: X has 23 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 46 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 69 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 92 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 115 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 138 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 161 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 184 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 207 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 230 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 253 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 276 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 299 features, but StandardScaler is expecting 345 features as input.\n",
      "Error displaying frame: X has 322 features, but StandardScaler is expecting 345 features as input.\n",
      "Updated Classification to: tutupmuka with confidence 89.16%\n",
      "Updated Classification to: tutupmuka with confidence 90.96%\n",
      "Updated Classification to: tutupmuka with confidence 92.59%\n",
      "Updated Classification to: tutupmuka with confidence 96.02%\n",
      "Updated Classification to: tutupmuka with confidence 97.81%\n",
      "Updated Classification to: tutupmuka with confidence 91.43%\n",
      "Updated Classification to: tutupmuka with confidence 76.97%\n",
      "Updated Classification to: tutupmuka with confidence 81.99%\n",
      "Updated Classification to: tutupmuka with confidence 81.26%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[0;32m---> 31\u001b[0m     frame_cropped \u001b[38;5;241m=\u001b[39m \u001b[43mcropped_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# FIX 1: Skip if initial cropping failed (frame_cropped is None)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame_cropped \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mcropped_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcropped_frame\u001b[39m(frame):\n\u001b[0;32m----> 2\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_yolo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\n\u001b[1;32m      4\u001b[0m     plotted_frame \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot() \n",
      "File \u001b[0;32m/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/ultralytics/engine/model.py:187\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    160\u001b[0m     source: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    163\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/ultralytics/engine/model.py:557\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:229\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:332\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 332\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m/mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:171\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    168\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(im)  \u001b[38;5;66;03m# contiguous\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     im \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(im)\n\u001b[0;32m--> 171\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mhalf() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;28;01melse\u001b[39;00m im\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# uint8 to fp16/32\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_tensor:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# cap = cv2.VideoCapture('http://192.168.100.197:5000/video')\n",
    "cap = cv2.VideoCapture(f'../../assets/test/vidio_test_1.mp4')\n",
    "# cap = cv2.VideoCapture(f'../../assets/dataset/v2/nodong_8.mp4')\n",
    "\n",
    "frames = []\n",
    "detected = []\n",
    "\n",
    "# ==== EXTENDED FEATURE ====\n",
    "face_direction = []\n",
    "face_shown = []\n",
    "hand_shown = []\n",
    "\n",
    "undetectable_image = None\n",
    "final_layout = None \n",
    "\n",
    "global_class = \"\"\n",
    "counter_detection = 0\n",
    "\n",
    "saved = 0\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "else:\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            start_time = time.time()\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_cropped = cropped_frame(frame)\n",
    "                \n",
    "                # FIX 1: Skip if initial cropping failed (frame_cropped is None)\n",
    "                if frame_cropped is None:\n",
    "                    print(\"Skipping frame: cropped_frame returned None.\")\n",
    "                    continue\n",
    "\n",
    "                # Prepare the current frame for side display (resized) and an empty list for detection\n",
    "                current_small_frame = cv2.resize(frame_cropped, (SMALL_SIZE))\n",
    "                current_detection_list = []\n",
    "                detection_successful = False\n",
    "\n",
    "                try:\n",
    "                    # Attempt Mediapipe detection\n",
    "                    frames_mp = cv2.cvtColor(frame_cropped, cv2.COLOR_BGR2RGB)\n",
    "                    frames_mp.flags.writeable = False\n",
    "                    \n",
    "                    results = holistic.process(frames_mp)\n",
    "                    frames_mp.flags.writeable = True\n",
    "                    frames_mp = cv2.cvtColor(frame_cropped, cv2.COLOR_RGB2BGR) \n",
    "\n",
    "                    # Extract landmarks if available\n",
    "                    if results.pose_landmarks:\n",
    "                        nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "                        ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "                        ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "                        wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                        wrist_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "                        current_detection_list.append(nose)\n",
    "                        current_detection_list.append(ear_l)\n",
    "                        current_detection_list.append(ear_r)\n",
    "                        current_detection_list.append(wrist_r)\n",
    "                        current_detection_list.append(wrist_l)\n",
    "                        detection_successful = True\n",
    "                    else:\n",
    "                        raise ValueError(\"No pose landmarks detected.\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    # Detection failed (Mediapipe error or no landmarks found)\n",
    "                    error_text = f\"No detection: {e}\"\n",
    "                    cv2.putText(frame_cropped, error_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    \n",
    "                if detection_successful:\n",
    "                    # This block is now ONLY executed if detection_successful is True\n",
    "                    extended_feature = get_extFeature_value({\"nose\": nose, \"ear_r\": ear_r, \"ear_l\": ear_l, \"wrist_r\": wrist_r, \"wrist_l\": wrist_l})\n",
    "\n",
    "                    if len(frames) < NUM_FRAMES:\n",
    "                        frames.append(current_small_frame)\n",
    "\n",
    "                        face_direction.append(extended_feature[0])\n",
    "                        face_shown.append(extended_feature[1])\n",
    "                        hand_shown.append(extended_feature[2])\n",
    "\n",
    "                        detected.append(current_detection_list)\n",
    "                    else:\n",
    "                        frames.pop(0)\n",
    "\n",
    "                        face_direction.pop(0)\n",
    "                        face_shown.pop(0)\n",
    "                        hand_shown.pop(0)\n",
    "\n",
    "                        detected.pop(0)\n",
    "                        \n",
    "                        frames.append(current_small_frame)\n",
    "\n",
    "                        face_direction.append(extended_feature[0])\n",
    "                        face_shown.append(extended_feature[1])\n",
    "                        hand_shown.append(extended_feature[2])\n",
    "\n",
    "                        detected.append(current_detection_list)\n",
    "                else:\n",
    "                    print(f\"Skipping frame {time.time() - start_time}ms: No pose landmarks detected.\")\n",
    "                    pass\n",
    "                \n",
    "                # --- LAYOUT AND DISPLAY LOGIC ---\n",
    "                if len(frames) == NUM_FRAMES:\n",
    "                    stacked1_frames = np.vstack((frames[0], frames[1], frames[2]))\n",
    "                    stacked2_frames = np.vstack((frames[3], frames[4], frames[5]))\n",
    "                    stacked3_frames = np.vstack((frames[6], frames[7], frames[8]))\n",
    "                    stacked4_frames = np.vstack((frames[9], frames[10], frames[11]))\n",
    "                    stacked5_frames = np.vstack((frames[12], frames[13], frames[14]))\n",
    "\n",
    "                    stacked1_frames = cv2.resize(stacked1_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked2_frames = cv2.resize(stacked2_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked3_frames = cv2.resize(stacked3_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked4_frames = cv2.resize(stacked4_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked5_frames = cv2.resize(stacked5_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "\n",
    "                    # All components are guaranteed to be valid images here\n",
    "                    final_layout = np.hstack((frame_cropped, stacked1_frames, stacked2_frames, stacked3_frames, stacked4_frames, stacked5_frames))\n",
    "                else:\n",
    "                    final_layout = frame_cropped\n",
    "                \n",
    "                if final_layout is not None and final_layout.size > 0:\n",
    "                    flat_detected = [landmark for sublist in detected for landmark in sublist]\n",
    "                    # print(len(flat_detected)) # Should always be 5 * NUM_FRAMES when full\n",
    "\n",
    "                    if len(flat_detected) > 0:\n",
    "                        landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "                        landmark_list.landmark.extend(flat_detected)\n",
    "                    \n",
    "                    try:\n",
    "                        # motion_row = []\n",
    "                        \n",
    "                        # for lv in landmark_list.landmark:\n",
    "                        #     motion_row.extend([lv.x, lv.y, lv.z, lv.visibility])\n",
    "\n",
    "                        counter = 0\n",
    "                        featurePer_frames = 0\n",
    "                        motion_row = []\n",
    "\n",
    "                        for lndmrk in landmark_list.landmark:\n",
    "                            motion_row.append(lndmrk.x)\n",
    "                            motion_row.append(lndmrk.y)\n",
    "                            motion_row.append(lndmrk.z)\n",
    "                            motion_row.append(lndmrk.visibility)\n",
    "                            counter += 1\n",
    "\n",
    "                            if counter % 5 == 0:\n",
    "                                motion_row.append(face_direction[featurePer_frames])\n",
    "                                motion_row.append(face_shown[featurePer_frames])\n",
    "                                motion_row.append(hand_shown[featurePer_frames])\n",
    "                                featurePer_frames += 1\n",
    "\n",
    "                        motion_row = list(np.array(motion_row))\n",
    "                        class_labels = model_pred.classes_\n",
    "\n",
    "                        motion_detected = pd.DataFrame([motion_row])\n",
    "                        probabilities = model_pred.predict_proba(motion_detected)[0]\n",
    "\n",
    "                        max_prob_index = np.argmax(probabilities)\n",
    "                        confidence_score = probabilities[max_prob_index] * 100 # Convert to percentage\n",
    "\n",
    "                        motion_class = class_labels[max_prob_index]\n",
    "                        confidence_text = f'{confidence_score:.2f}%'\n",
    "\n",
    "                        global_class, confidence, counter_detection = classficiation_missing_calculated(motion_class, confidence_score, counter_detection, global_class)\n",
    "\n",
    "\n",
    "                        # motion_class = model_pred.predict(motion_detected)[0]\n",
    "\n",
    "                        warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "                        # Update the text displayed on the video\n",
    "                        try:\n",
    "                            text_to_display = f'Class: {global_class} | Confidence: {confidence:.2f}%'\n",
    "                            draw_text_with_background(\n",
    "                                final_layout, \n",
    "                                text_to_display, \n",
    "                                ORG, \n",
    "                                FONT, \n",
    "                                FONT_SCALE, \n",
    "                                TEXT_COLOR, \n",
    "                                BLUE_COLOR, \n",
    "                                TEXT_THICKNESS\n",
    "                            )\n",
    "                        except:\n",
    "                            text_to_display = f'Class: {motion_class} | Confidence: {confidence_text}'\n",
    "                            draw_text_with_background(\n",
    "                                final_layout, \n",
    "                                text_to_display, \n",
    "                                ORG, \n",
    "                                FONT, \n",
    "                                FONT_SCALE, \n",
    "                                TEXT_COLOR, \n",
    "                                RED_COLOR, \n",
    "                                TEXT_THICKNESS\n",
    "                            )\n",
    "\n",
    "                        cv2.imshow('Video', final_layout)\n",
    "\n",
    "                        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "                        if key == ord('q'):\n",
    "                            break\n",
    "                    \n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error displaying frame: {e}\")\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08444e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
