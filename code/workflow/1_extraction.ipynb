{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d8090fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "FIXED_SIZE = 640\n",
    "SMALL_SIZE = 150, 150\n",
    "NUM_FRAMES = 15\n",
    "model = YOLO(\"../../model/yolo/yolo12n.pt\") \n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142cd389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropped_frame(frame):\n",
    "    results = model(frame, classes=[0])\n",
    "    boxes = results[0].boxes\n",
    "    plotted_frame = results[0].plot() \n",
    "\n",
    "    try:\n",
    "        if len(boxes) > 0:\n",
    "            # Get the bounding box coordinates for the first detected object\n",
    "            x1, y1, x2, y2 = boxes.xyxy[0].cpu().numpy().astype(int)\n",
    "\n",
    "            cropped_frame = frame[y1:y2, x1:x2]\n",
    "            crop_h, crop_w = cropped_frame.shape[:2]\n",
    "\n",
    "            # We want to fit the largest dimension (width or height) to the FIXED_SIZE\n",
    "            scale = FIXED_SIZE / max(crop_w, crop_h)\n",
    "            new_w = int(crop_w * scale)\n",
    "            new_h = int(crop_h * scale)\n",
    "\n",
    "            # Resize the cropped frame to the new dimensions\n",
    "            resized_img = cv2.resize(cropped_frame, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Background frame, that are not filled with boxes\n",
    "            final_frame = np.full((FIXED_SIZE, FIXED_SIZE, 3), 255, dtype=np.uint8)\n",
    "            \n",
    "            # dw and dh are the space left over after placing the image\n",
    "            dw = FIXED_SIZE - new_w\n",
    "            dh = FIXED_SIZE - new_h\n",
    "\n",
    "            # Calculate the starting position (top-left corner) for centering\n",
    "            top = dh // 2\n",
    "            bottom = top + new_h\n",
    "            left = dw // 2\n",
    "            right = left + new_w\n",
    "\n",
    "            final_frame[top:bottom, left:right] = resized_img\n",
    "\n",
    "            return final_frame\n",
    "\n",
    "        else:\n",
    "            print(\"No objects detected in the image.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6904e6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 (no detections), 32.6ms\n",
      "Speed: 2.5ms preprocess, 32.6ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 (no detections), 28.9ms\n",
      "Speed: 3.0ms preprocess, 28.9ms inference, 2.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 (no detections), 23.9ms\n",
      "Speed: 2.0ms preprocess, 23.9ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760686200.273707     867 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1760686200.283177    4958 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: llvmpipe (LLVM 20.1.2, 256 bits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 320x640 (no detections), 23.2ms\n",
      "Speed: 1.7ms preprocess, 23.2ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 (no detections), 21.5ms\n",
      "Speed: 2.2ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 (no detections), 20.6ms\n",
      "Speed: 1.7ms preprocess, 20.6ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 1 person, 20.9ms\n",
      "Speed: 1.9ms preprocess, 20.9ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760686200.644494    4948 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760686200.811263    4950 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760686200.823889    4955 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760686200.825320    4957 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760686200.829527    4956 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n",
      "0: 320x640 1 person, 19.9ms\n",
      "Speed: 1.9ms preprocess, 19.9ms inference, 3.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "10\n",
      "\n",
      "0: 320x640 1 person, 18.7ms\n",
      "Speed: 2.2ms preprocess, 18.7ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1760686200.846802    4948 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760686200.857107    4952 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760686200.859812    4947 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "\n",
      "0: 320x640 1 person, 16.6ms\n",
      "Speed: 2.5ms preprocess, 16.6ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "20\n",
      "\n",
      "0: 320x640 1 person, 18.5ms\n",
      "Speed: 2.1ms preprocess, 18.5ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "25\n",
      "\n",
      "0: 320x640 (no detections), 18.9ms\n",
      "Speed: 1.9ms preprocess, 18.9ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 (no detections), 37.8ms\n",
      "Speed: 2.3ms preprocess, 37.8ms inference, 4.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 (no detections), 17.2ms\n",
      "Speed: 2.1ms preprocess, 17.2ms inference, 1.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 (no detections), 19.9ms\n",
      "Speed: 1.6ms preprocess, 19.9ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 (no detections), 19.8ms\n",
      "Speed: 1.6ms preprocess, 19.8ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 (no detections), 21.8ms\n",
      "Speed: 1.6ms preprocess, 21.8ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 (no detections), 18.3ms\n",
      "Speed: 2.2ms preprocess, 18.3ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "No objects detected in the image.\n",
      "Skipping frame: cropped_frame returned None.\n",
      "\n",
      "0: 320x640 1 person, 18.9ms\n",
      "Speed: 1.6ms preprocess, 18.9ms inference, 3.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "30\n",
      "\n",
      "0: 320x640 1 person, 21.0ms\n",
      "Speed: 1.8ms preprocess, 21.0ms inference, 4.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "35\n",
      "\n",
      "0: 320x640 1 person, 31.0ms\n",
      "Speed: 2.3ms preprocess, 31.0ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "40\n",
      "\n",
      "0: 320x640 1 person, 19.4ms\n",
      "Speed: 2.4ms preprocess, 19.4ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "45\n",
      "\n",
      "0: 320x640 1 person, 19.4ms\n",
      "Speed: 2.0ms preprocess, 19.4ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "50\n",
      "\n",
      "0: 320x640 1 person, 18.7ms\n",
      "Speed: 1.7ms preprocess, 18.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "55\n",
      "\n",
      "0: 320x640 1 person, 18.3ms\n",
      "Speed: 1.5ms preprocess, 18.3ms inference, 3.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "60\n",
      "\n",
      "0: 320x640 1 person, 19.9ms\n",
      "Speed: 1.9ms preprocess, 19.9ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "65\n",
      "\n",
      "0: 320x640 1 person, 17.4ms\n",
      "Speed: 1.9ms preprocess, 17.4ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "70\n",
      "\n",
      "0: 320x640 1 person, 17.8ms\n",
      "Speed: 1.7ms preprocess, 17.8ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.8ms\n",
      "Speed: 1.5ms preprocess, 17.8ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 30.2ms\n",
      "Speed: 1.9ms preprocess, 30.2ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 20.8ms\n",
      "Speed: 2.3ms preprocess, 20.8ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.7ms\n",
      "Speed: 2.1ms preprocess, 16.7ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.4ms\n",
      "Speed: 1.8ms preprocess, 17.4ms inference, 3.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 23.7ms\n",
      "Speed: 2.3ms preprocess, 23.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.4ms\n",
      "Speed: 1.8ms preprocess, 18.4ms inference, 6.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.1ms\n",
      "Speed: 1.7ms preprocess, 16.1ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.0ms\n",
      "Speed: 1.7ms preprocess, 18.0ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.6ms\n",
      "Speed: 2.2ms preprocess, 21.6ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.9ms\n",
      "Speed: 1.8ms preprocess, 19.9ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.0ms\n",
      "Speed: 1.6ms preprocess, 19.0ms inference, 4.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.0ms\n",
      "Speed: 1.9ms preprocess, 18.0ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.2ms\n",
      "Speed: 1.8ms preprocess, 18.2ms inference, 3.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.8ms\n",
      "Speed: 2.0ms preprocess, 16.8ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 36.1ms\n",
      "Speed: 1.7ms preprocess, 36.1ms inference, 6.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.1ms\n",
      "Speed: 1.4ms preprocess, 17.1ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.4ms\n",
      "Speed: 1.9ms preprocess, 18.4ms inference, 3.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 20.7ms\n",
      "Speed: 2.3ms preprocess, 20.7ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.4ms\n",
      "Speed: 1.8ms preprocess, 17.4ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 31.6ms\n",
      "Speed: 2.4ms preprocess, 31.6ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 24.6ms\n",
      "Speed: 1.9ms preprocess, 24.6ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 27.4ms\n",
      "Speed: 2.6ms preprocess, 27.4ms inference, 5.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.2ms\n",
      "Speed: 1.7ms preprocess, 18.2ms inference, 4.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.1ms\n",
      "Speed: 2.1ms preprocess, 18.1ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.4ms\n",
      "Speed: 1.5ms preprocess, 18.4ms inference, 2.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 33.6ms\n",
      "Speed: 1.9ms preprocess, 33.6ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 45.3ms\n",
      "Speed: 1.5ms preprocess, 45.3ms inference, 7.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 29.9ms\n",
      "Speed: 2.0ms preprocess, 29.9ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 24.0ms\n",
      "Speed: 2.4ms preprocess, 24.0ms inference, 3.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 29.8ms\n",
      "Speed: 2.2ms preprocess, 29.8ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 20.3ms\n",
      "Speed: 2.8ms preprocess, 20.3ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.8ms\n",
      "Speed: 2.2ms preprocess, 18.8ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.5ms\n",
      "Speed: 1.5ms preprocess, 19.5ms inference, 3.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.3ms\n",
      "Speed: 1.9ms preprocess, 21.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.2ms\n",
      "Speed: 1.6ms preprocess, 19.2ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.8ms\n",
      "Speed: 1.8ms preprocess, 21.8ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.7ms\n",
      "Speed: 2.2ms preprocess, 17.7ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.6ms\n",
      "Speed: 2.2ms preprocess, 17.6ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.8ms\n",
      "Speed: 2.2ms preprocess, 17.8ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 20.3ms\n",
      "Speed: 2.4ms preprocess, 20.3ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.6ms\n",
      "Speed: 1.6ms preprocess, 18.6ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.5ms\n",
      "Speed: 1.7ms preprocess, 17.5ms inference, 3.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.3ms\n",
      "Speed: 2.0ms preprocess, 19.3ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 34.1ms\n",
      "Speed: 2.1ms preprocess, 34.1ms inference, 6.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 23.9ms\n",
      "Speed: 2.5ms preprocess, 23.9ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.3ms\n",
      "Speed: 1.7ms preprocess, 21.3ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.4ms\n",
      "Speed: 1.6ms preprocess, 17.4ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.9ms\n",
      "Speed: 1.7ms preprocess, 18.9ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.9ms\n",
      "Speed: 1.8ms preprocess, 17.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.8ms\n",
      "Speed: 1.8ms preprocess, 18.8ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 3.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.8ms\n",
      "Speed: 1.8ms preprocess, 18.8ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.9ms\n",
      "Speed: 2.3ms preprocess, 18.9ms inference, 3.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.5ms\n",
      "Speed: 1.9ms preprocess, 16.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.9ms\n",
      "Speed: 2.1ms preprocess, 18.9ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.3ms\n",
      "Speed: 2.0ms preprocess, 17.3ms inference, 3.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 40.1ms\n",
      "Speed: 2.0ms preprocess, 40.1ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.7ms\n",
      "Speed: 1.7ms preprocess, 17.7ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.3ms\n",
      "Speed: 1.8ms preprocess, 16.3ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.6ms\n",
      "Speed: 1.6ms preprocess, 16.6ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.7ms\n",
      "Speed: 2.4ms preprocess, 16.7ms inference, 3.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.1ms\n",
      "Speed: 2.2ms preprocess, 19.1ms inference, 3.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 22.2ms\n",
      "Speed: 2.0ms preprocess, 22.2ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 20.8ms\n",
      "Speed: 2.4ms preprocess, 20.8ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.7ms\n",
      "Speed: 1.6ms preprocess, 17.7ms inference, 3.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 15.8ms\n",
      "Speed: 2.3ms preprocess, 15.8ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.9ms\n",
      "Speed: 2.1ms preprocess, 19.9ms inference, 3.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.9ms\n",
      "Speed: 1.9ms preprocess, 21.9ms inference, 3.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.9ms\n",
      "Speed: 2.2ms preprocess, 18.9ms inference, 4.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.3ms\n",
      "Speed: 1.9ms preprocess, 18.3ms inference, 6.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 31.9ms\n",
      "Speed: 2.0ms preprocess, 31.9ms inference, 10.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.5ms\n",
      "Speed: 1.6ms preprocess, 17.5ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 22.3ms\n",
      "Speed: 2.5ms preprocess, 22.3ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 28.1ms\n",
      "Speed: 2.1ms preprocess, 28.1ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 15.3ms\n",
      "Speed: 2.3ms preprocess, 15.3ms inference, 3.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.4ms\n",
      "Speed: 1.6ms preprocess, 18.4ms inference, 4.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 24.6ms\n",
      "Speed: 2.6ms preprocess, 24.6ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 24.7ms\n",
      "Speed: 2.3ms preprocess, 24.7ms inference, 4.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 26.8ms\n",
      "Speed: 1.6ms preprocess, 26.8ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 26.5ms\n",
      "Speed: 2.4ms preprocess, 26.5ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.9ms\n",
      "Speed: 2.2ms preprocess, 21.9ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 20.7ms\n",
      "Speed: 2.2ms preprocess, 20.7ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.9ms\n",
      "Speed: 1.9ms preprocess, 18.9ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 22.3ms\n",
      "Speed: 2.5ms preprocess, 22.3ms inference, 3.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 22.3ms\n",
      "Speed: 2.1ms preprocess, 22.3ms inference, 3.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 23.8ms\n",
      "Speed: 3.3ms preprocess, 23.8ms inference, 4.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 29.8ms\n",
      "Speed: 5.6ms preprocess, 29.8ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 23.4ms\n",
      "Speed: 2.7ms preprocess, 23.4ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 27.6ms\n",
      "Speed: 2.2ms preprocess, 27.6ms inference, 3.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 30.4ms\n",
      "Speed: 2.1ms preprocess, 30.4ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 24.8ms\n",
      "Speed: 2.6ms preprocess, 24.8ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 27.0ms\n",
      "Speed: 3.1ms preprocess, 27.0ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 27.6ms\n",
      "Speed: 2.3ms preprocess, 27.6ms inference, 5.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 22.3ms\n",
      "Speed: 2.0ms preprocess, 22.3ms inference, 3.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 31.1ms\n",
      "Speed: 1.7ms preprocess, 31.1ms inference, 3.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.9ms\n",
      "Speed: 1.6ms preprocess, 21.9ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 32.8ms\n",
      "Speed: 2.3ms preprocess, 32.8ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.1ms\n",
      "Speed: 1.7ms preprocess, 19.1ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 29.5ms\n",
      "Speed: 3.1ms preprocess, 29.5ms inference, 3.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 24.0ms\n",
      "Speed: 1.7ms preprocess, 24.0ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.8ms\n",
      "Speed: 2.2ms preprocess, 21.8ms inference, 3.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 28.4ms\n",
      "Speed: 3.0ms preprocess, 28.4ms inference, 4.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 2 persons, 22.9ms\n",
      "Speed: 2.4ms preprocess, 22.9ms inference, 3.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 2 persons, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 4.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 2 persons, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 4.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 2 persons, 23.6ms\n",
      "Speed: 1.9ms preprocess, 23.6ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 2 persons, 20.7ms\n",
      "Speed: 2.1ms preprocess, 20.7ms inference, 3.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 2 persons, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 51.1ms\n",
      "Speed: 2.4ms preprocess, 51.1ms inference, 3.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 25.4ms\n",
      "Speed: 3.1ms preprocess, 25.4ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 30.7ms\n",
      "Speed: 2.1ms preprocess, 30.7ms inference, 3.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 32.6ms\n",
      "Speed: 2.3ms preprocess, 32.6ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 25.5ms\n",
      "Speed: 2.1ms preprocess, 25.5ms inference, 7.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.4ms\n",
      "Speed: 1.6ms preprocess, 19.4ms inference, 4.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.3ms\n",
      "Speed: 2.2ms preprocess, 19.3ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.2ms\n",
      "Speed: 1.6ms preprocess, 21.2ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.7ms\n",
      "Speed: 1.7ms preprocess, 18.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 42.5ms\n",
      "Speed: 2.6ms preprocess, 42.5ms inference, 7.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 20.2ms\n",
      "Speed: 1.9ms preprocess, 20.2ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.5ms\n",
      "Speed: 1.8ms preprocess, 17.5ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.7ms\n",
      "Speed: 1.8ms preprocess, 19.7ms inference, 3.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.9ms\n",
      "Speed: 2.3ms preprocess, 19.9ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.3ms\n",
      "Speed: 1.7ms preprocess, 18.3ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 20.8ms\n",
      "Speed: 2.0ms preprocess, 20.8ms inference, 3.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.1ms\n",
      "Speed: 1.9ms preprocess, 21.1ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 32.0ms\n",
      "Speed: 1.6ms preprocess, 32.0ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.5ms\n",
      "Speed: 2.1ms preprocess, 18.5ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 22.7ms\n",
      "Speed: 1.8ms preprocess, 22.7ms inference, 3.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.7ms\n",
      "Speed: 2.4ms preprocess, 17.7ms inference, 4.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.5ms\n",
      "Speed: 2.0ms preprocess, 19.5ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.7ms\n",
      "Speed: 2.0ms preprocess, 18.7ms inference, 7.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 20.5ms\n",
      "Speed: 1.9ms preprocess, 20.5ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.1ms\n",
      "Speed: 2.3ms preprocess, 21.1ms inference, 3.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.1ms\n",
      "Speed: 2.3ms preprocess, 21.1ms inference, 5.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.7ms\n",
      "Speed: 2.9ms preprocess, 17.7ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.9ms\n",
      "Speed: 2.1ms preprocess, 19.9ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.5ms\n",
      "Speed: 2.6ms preprocess, 18.5ms inference, 4.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.5ms\n",
      "Speed: 2.5ms preprocess, 17.5ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.5ms\n",
      "Speed: 2.1ms preprocess, 18.5ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.2ms\n",
      "Speed: 1.8ms preprocess, 17.2ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.0ms\n",
      "Speed: 2.2ms preprocess, 19.0ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.1ms\n",
      "Speed: 1.8ms preprocess, 19.1ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.3ms\n",
      "Speed: 1.7ms preprocess, 18.3ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.9ms\n",
      "Speed: 1.6ms preprocess, 17.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.7ms\n",
      "Speed: 1.7ms preprocess, 18.7ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.1ms\n",
      "Speed: 1.8ms preprocess, 18.1ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.5ms\n",
      "Speed: 2.2ms preprocess, 17.5ms inference, 3.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.8ms\n",
      "Speed: 1.8ms preprocess, 17.8ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.6ms\n",
      "Speed: 1.8ms preprocess, 16.6ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.5ms\n",
      "Speed: 2.1ms preprocess, 17.5ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.4ms\n",
      "Speed: 2.0ms preprocess, 18.4ms inference, 7.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.5ms\n",
      "Speed: 2.1ms preprocess, 17.5ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.6ms\n",
      "Speed: 2.0ms preprocess, 17.6ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.3ms\n",
      "Speed: 1.9ms preprocess, 21.3ms inference, 3.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.7ms\n",
      "Speed: 2.1ms preprocess, 19.7ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.8ms\n",
      "Speed: 2.2ms preprocess, 17.8ms inference, 3.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.6ms\n",
      "Speed: 2.3ms preprocess, 18.6ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.5ms\n",
      "Speed: 2.1ms preprocess, 16.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.4ms\n",
      "Speed: 2.0ms preprocess, 18.4ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 20.5ms\n",
      "Speed: 1.8ms preprocess, 20.5ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.3ms\n",
      "Speed: 2.0ms preprocess, 21.3ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 7.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.5ms\n",
      "Speed: 2.1ms preprocess, 17.5ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 3.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.5ms\n",
      "Speed: 2.2ms preprocess, 19.5ms inference, 6.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.5ms\n",
      "Speed: 2.4ms preprocess, 17.5ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.6ms\n",
      "Speed: 2.0ms preprocess, 16.6ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.0ms\n",
      "Speed: 2.2ms preprocess, 18.0ms inference, 6.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.0ms\n",
      "Speed: 2.5ms preprocess, 19.0ms inference, 3.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.6ms\n",
      "Speed: 2.2ms preprocess, 16.6ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.3ms\n",
      "Speed: 2.1ms preprocess, 16.3ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 18.7ms\n",
      "Speed: 2.1ms preprocess, 18.7ms inference, 5.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 15.8ms\n",
      "Speed: 2.6ms preprocess, 15.8ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 15.8ms\n",
      "Speed: 2.0ms preprocess, 15.8ms inference, 4.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.0ms\n",
      "Speed: 2.3ms preprocess, 19.0ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.8ms\n",
      "Speed: 2.4ms preprocess, 19.8ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.1ms\n",
      "Speed: 2.2ms preprocess, 17.1ms inference, 6.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 15.8ms\n",
      "Speed: 1.8ms preprocess, 15.8ms inference, 4.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 15.4ms\n",
      "Speed: 2.2ms preprocess, 15.4ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 31.4ms\n",
      "Speed: 2.0ms preprocess, 31.4ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 37.9ms\n",
      "Speed: 2.1ms preprocess, 37.9ms inference, 6.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.8ms\n",
      "Speed: 2.3ms preprocess, 17.8ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 26.2ms\n",
      "Speed: 2.1ms preprocess, 26.2ms inference, 2.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 31.0ms\n",
      "Speed: 2.5ms preprocess, 31.0ms inference, 4.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.9ms\n",
      "Speed: 2.0ms preprocess, 16.9ms inference, 2.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 16.8ms\n",
      "Speed: 2.2ms preprocess, 16.8ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 27.2ms\n",
      "Speed: 2.6ms preprocess, 27.2ms inference, 2.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.1ms\n",
      "Speed: 2.4ms preprocess, 17.1ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 17.1ms\n",
      "Speed: 2.3ms preprocess, 17.1ms inference, 2.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 19.8ms\n",
      "Speed: 2.2ms preprocess, 19.8ms inference, 2.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n",
      "\n",
      "0: 320x640 1 person, 40.0ms\n",
      "Speed: 2.3ms preprocess, 40.0ms inference, 8.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('../../assets/dump/celinguk.mp4')\n",
    "\n",
    "frames = []\n",
    "detected = []\n",
    "undetectable_image = None\n",
    "final_layout = None \n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "else:\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            start_time = time.time()\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_cropped = cropped_frame(frame)\n",
    "                \n",
    "                # FIX 1: Skip if initial cropping failed (frame_cropped is None)\n",
    "                if frame_cropped is None:\n",
    "                    print(\"Skipping frame: cropped_frame returned None.\")\n",
    "                    continue\n",
    "\n",
    "                # Prepare the current frame for side display (resized) and an empty list for detection\n",
    "                current_small_frame = cv2.resize(frame_cropped, (SMALL_SIZE))\n",
    "                current_detection_list = []\n",
    "                detection_successful = False\n",
    "\n",
    "                try:\n",
    "                    # Attempt Mediapipe detection\n",
    "                    frames_mp = cv2.cvtColor(frame_cropped, cv2.COLOR_BGR2RGB)\n",
    "                    frames_mp.flags.writeable = False\n",
    "                    \n",
    "                    results = holistic.process(frames_mp)\n",
    "                    frames_mp.flags.writeable = True\n",
    "                    frames_mp = cv2.cvtColor(frame_cropped, cv2.COLOR_RGB2BGR) \n",
    "\n",
    "                    # Extract landmarks if available\n",
    "                    if results.pose_landmarks:\n",
    "                        current_detection_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE])\n",
    "                        current_detection_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EYE])\n",
    "                        current_detection_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EYE])\n",
    "                        current_detection_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST])\n",
    "                        current_detection_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_WRIST])\n",
    "                        detection_successful = True\n",
    "                    else:\n",
    "                        raise ValueError(\"No pose landmarks detected.\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    # Detection failed (Mediapipe error or no landmarks found)\n",
    "                    error_text = f\"No detection: {e}\"\n",
    "                    cv2.putText(frame_cropped, error_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    \n",
    "                    \n",
    "                    placeholder_landmark = landmark_pb2.NormalizedLandmark(x=0.0, y=0.0, z=0.0, visibility=0.0)\n",
    "                    current_detection_list = [placeholder_landmark] * 5 \n",
    "                    detection_successful = True # Treat as 'successful' failure to maintain list length\n",
    "\n",
    "                # FIX 3: Always manage the lists (frames and detected) here, regardless of detection success\n",
    "                if len(frames) < NUM_FRAMES:\n",
    "                    frames.append(current_small_frame)\n",
    "                    detected.append(current_detection_list)\n",
    "                else:\n",
    "                    frames.pop(0)\n",
    "                    detected.pop(0)\n",
    "                    frames.append(current_small_frame)\n",
    "                    detected.append(current_detection_list)\n",
    "                \n",
    "                # --- LAYOUT AND DISPLAY LOGIC ---\n",
    "                if len(frames) == NUM_FRAMES:\n",
    "                    stacked1_frames = np.vstack((frames[0], frames[1], frames[2]))\n",
    "                    stacked2_frames = np.vstack((frames[3], frames[4], frames[5]))\n",
    "                    stacked3_frames = np.vstack((frames[6], frames[7], frames[8]))\n",
    "                    stacked4_frames = np.vstack((frames[9], frames[10], frames[11]))\n",
    "                    stacked5_frames = np.vstack((frames[12], frames[13], frames[14]))\n",
    "\n",
    "                    stacked1_frames = cv2.resize(stacked1_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked2_frames = cv2.resize(stacked2_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked3_frames = cv2.resize(stacked3_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked4_frames = cv2.resize(stacked4_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "                    stacked5_frames = cv2.resize(stacked5_frames, (SMALL_SIZE[0], FIXED_SIZE))\n",
    "\n",
    "                    # All components are guaranteed to be valid images here\n",
    "                    final_layout = np.hstack((frame_cropped, stacked1_frames, stacked2_frames, stacked3_frames, stacked4_frames, stacked5_frames))\n",
    "                else:\n",
    "                    final_layout = frame_cropped\n",
    "                \n",
    "                \n",
    "                if final_layout is not None and final_layout.size > 0:\n",
    "                    flat_detected = [landmark for sublist in detected for landmark in sublist]\n",
    "                    print(len(flat_detected)) # Should always be 5 * NUM_FRAMES when full\n",
    "\n",
    "                    if len(flat_detected) > 0:\n",
    "                        landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "                        landmark_list.landmark.extend(flat_detected)\n",
    "                    \n",
    "                    try:\n",
    "                        cv2.imshow('Video', final_layout)\n",
    "\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error displaying frame: {e}\")\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a79e1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'x1',\n",
       " 'y1',\n",
       " 'z1',\n",
       " 'v1',\n",
       " 'x2',\n",
       " 'y2',\n",
       " 'z2',\n",
       " 'v2',\n",
       " 'x3',\n",
       " 'y3',\n",
       " 'z3',\n",
       " 'v3',\n",
       " 'x4',\n",
       " 'y4',\n",
       " 'z4',\n",
       " 'v4',\n",
       " 'x5',\n",
       " 'y5',\n",
       " 'z5',\n",
       " 'v5',\n",
       " 'x6',\n",
       " 'y6',\n",
       " 'z6',\n",
       " 'v6',\n",
       " 'x7',\n",
       " 'y7',\n",
       " 'z7',\n",
       " 'v7',\n",
       " 'x8',\n",
       " 'y8',\n",
       " 'z8',\n",
       " 'v8',\n",
       " 'x9',\n",
       " 'y9',\n",
       " 'z9',\n",
       " 'v9',\n",
       " 'x10',\n",
       " 'y10',\n",
       " 'z10',\n",
       " 'v10',\n",
       " 'x11',\n",
       " 'y11',\n",
       " 'z11',\n",
       " 'v11',\n",
       " 'x12',\n",
       " 'y12',\n",
       " 'z12',\n",
       " 'v12',\n",
       " 'x13',\n",
       " 'y13',\n",
       " 'z13',\n",
       " 'v13',\n",
       " 'x14',\n",
       " 'y14',\n",
       " 'z14',\n",
       " 'v14',\n",
       " 'x15',\n",
       " 'y15',\n",
       " 'z15',\n",
       " 'v15',\n",
       " 'x16',\n",
       " 'y16',\n",
       " 'z16',\n",
       " 'v16',\n",
       " 'x17',\n",
       " 'y17',\n",
       " 'z17',\n",
       " 'v17',\n",
       " 'x18',\n",
       " 'y18',\n",
       " 'z18',\n",
       " 'v18',\n",
       " 'x19',\n",
       " 'y19',\n",
       " 'z19',\n",
       " 'v19',\n",
       " 'x20',\n",
       " 'y20',\n",
       " 'z20',\n",
       " 'v20',\n",
       " 'x21',\n",
       " 'y21',\n",
       " 'z21',\n",
       " 'v21',\n",
       " 'x22',\n",
       " 'y22',\n",
       " 'z22',\n",
       " 'v22',\n",
       " 'x23',\n",
       " 'y23',\n",
       " 'z23',\n",
       " 'v23',\n",
       " 'x24',\n",
       " 'y24',\n",
       " 'z24',\n",
       " 'v24',\n",
       " 'x25',\n",
       " 'y25',\n",
       " 'z25',\n",
       " 'v25',\n",
       " 'x26',\n",
       " 'y26',\n",
       " 'z26',\n",
       " 'v26',\n",
       " 'x27',\n",
       " 'y27',\n",
       " 'z27',\n",
       " 'v27',\n",
       " 'x28',\n",
       " 'y28',\n",
       " 'z28',\n",
       " 'v28',\n",
       " 'x29',\n",
       " 'y29',\n",
       " 'z29',\n",
       " 'v29',\n",
       " 'x30',\n",
       " 'y30',\n",
       " 'z30',\n",
       " 'v30',\n",
       " 'x31',\n",
       " 'y31',\n",
       " 'z31',\n",
       " 'v31',\n",
       " 'x32',\n",
       " 'y32',\n",
       " 'z32',\n",
       " 'v32',\n",
       " 'x33',\n",
       " 'y33',\n",
       " 'z33',\n",
       " 'v33',\n",
       " 'x34',\n",
       " 'y34',\n",
       " 'z34',\n",
       " 'v34',\n",
       " 'x35',\n",
       " 'y35',\n",
       " 'z35',\n",
       " 'v35',\n",
       " 'x36',\n",
       " 'y36',\n",
       " 'z36',\n",
       " 'v36',\n",
       " 'x37',\n",
       " 'y37',\n",
       " 'z37',\n",
       " 'v37',\n",
       " 'x38',\n",
       " 'y38',\n",
       " 'z38',\n",
       " 'v38',\n",
       " 'x39',\n",
       " 'y39',\n",
       " 'z39',\n",
       " 'v39',\n",
       " 'x40',\n",
       " 'y40',\n",
       " 'z40',\n",
       " 'v40',\n",
       " 'x41',\n",
       " 'y41',\n",
       " 'z41',\n",
       " 'v41',\n",
       " 'x42',\n",
       " 'y42',\n",
       " 'z42',\n",
       " 'v42',\n",
       " 'x43',\n",
       " 'y43',\n",
       " 'z43',\n",
       " 'v43',\n",
       " 'x44',\n",
       " 'y44',\n",
       " 'z44',\n",
       " 'v44',\n",
       " 'x45',\n",
       " 'y45',\n",
       " 'z45',\n",
       " 'v45',\n",
       " 'x46',\n",
       " 'y46',\n",
       " 'z46',\n",
       " 'v46',\n",
       " 'x47',\n",
       " 'y47',\n",
       " 'z47',\n",
       " 'v47',\n",
       " 'x48',\n",
       " 'y48',\n",
       " 'z48',\n",
       " 'v48',\n",
       " 'x49',\n",
       " 'y49',\n",
       " 'z49',\n",
       " 'v49',\n",
       " 'x50',\n",
       " 'y50',\n",
       " 'z50',\n",
       " 'v50',\n",
       " 'x51',\n",
       " 'y51',\n",
       " 'z51',\n",
       " 'v51',\n",
       " 'x52',\n",
       " 'y52',\n",
       " 'z52',\n",
       " 'v52',\n",
       " 'x53',\n",
       " 'y53',\n",
       " 'z53',\n",
       " 'v53',\n",
       " 'x54',\n",
       " 'y54',\n",
       " 'z54',\n",
       " 'v54',\n",
       " 'x55',\n",
       " 'y55',\n",
       " 'z55',\n",
       " 'v55',\n",
       " 'x56',\n",
       " 'y56',\n",
       " 'z56',\n",
       " 'v56',\n",
       " 'x57',\n",
       " 'y57',\n",
       " 'z57',\n",
       " 'v57',\n",
       " 'x58',\n",
       " 'y58',\n",
       " 'z58',\n",
       " 'v58',\n",
       " 'x59',\n",
       " 'y59',\n",
       " 'z59',\n",
       " 'v59',\n",
       " 'x60',\n",
       " 'y60',\n",
       " 'z60',\n",
       " 'v60',\n",
       " 'x61',\n",
       " 'y61',\n",
       " 'z61',\n",
       " 'v61',\n",
       " 'x62',\n",
       " 'y62',\n",
       " 'z62',\n",
       " 'v62',\n",
       " 'x63',\n",
       " 'y63',\n",
       " 'z63',\n",
       " 'v63',\n",
       " 'x64',\n",
       " 'y64',\n",
       " 'z64',\n",
       " 'v64',\n",
       " 'x65',\n",
       " 'y65',\n",
       " 'z65',\n",
       " 'v65',\n",
       " 'x66',\n",
       " 'y66',\n",
       " 'z66',\n",
       " 'v66',\n",
       " 'x67',\n",
       " 'y67',\n",
       " 'z67',\n",
       " 'v67',\n",
       " 'x68',\n",
       " 'y68',\n",
       " 'z68',\n",
       " 'v68',\n",
       " 'x69',\n",
       " 'y69',\n",
       " 'z69',\n",
       " 'v69',\n",
       " 'x70',\n",
       " 'y70',\n",
       " 'z70',\n",
       " 'v70',\n",
       " 'x71',\n",
       " 'y71',\n",
       " 'z71',\n",
       " 'v71',\n",
       " 'x72',\n",
       " 'y72',\n",
       " 'z72',\n",
       " 'v72',\n",
       " 'x73',\n",
       " 'y73',\n",
       " 'z73',\n",
       " 'v73',\n",
       " 'x74',\n",
       " 'y74',\n",
       " 'z74',\n",
       " 'v74',\n",
       " 'x75',\n",
       " 'y75',\n",
       " 'z75',\n",
       " 'v75']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_coords = len(landmark_list.landmark)\n",
    "used_coords\n",
    "frames = 1\n",
    "\n",
    "landmarks = ['class']\n",
    "for val in range(1, used_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]\n",
    "    # if val % 5 == 0:\n",
    "    #     landmarks += ['face_d{}'.format(frames), 'face_s{}'.format(frames), 'hand_s{}'.format(frames)]\n",
    "    frames += 1\n",
    "\n",
    "\n",
    "landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e13f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_csv = '../../dataset/csv/dataset_v1.csv'\n",
    "os.makedirs(os.path.dirname(file_csv), exist_ok=True)\n",
    "\n",
    "with open(file_csv, mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192af9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
