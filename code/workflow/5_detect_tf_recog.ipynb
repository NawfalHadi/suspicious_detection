{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60292c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 01:58:32.710167: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-21 01:58:32.722158: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763661512.732455  319762 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763661512.735605  319762 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1763661512.745743  319762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763661512.745761  319762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763661512.745762  319762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763661512.745763  319762 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-21 01:58:32.749159: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ruvne/anaconda3/envs/py310/lib/python3.10/site-packages/face_recognition_models/__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import pickle\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e441f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # BASE_DIR: lokasi file ini (kalau .py) atau CWD (kalau di Jupyter)\n",
    "    BASE_DIR: Path = (\n",
    "        Path(__file__).resolve().parent\n",
    "        if \"__file__\" in globals()\n",
    "        else Path(os.getcwd())\n",
    "    )\n",
    "\n",
    "    # YOLO models (.pt atau .engine)\n",
    "    PERSON_MODEL_PATH: str = str(BASE_DIR / \"model\" / \"yolo\" / \"yolo12n.pt\")\n",
    "    WEAPON_MODEL_PATH: str = str(BASE_DIR / \"model\" / \"yolo\" / \"yolov12n-weapon.pt\")\n",
    "    FACE_MODEL_PATH:   str = str(BASE_DIR / \"model\" / \"yolo\" / \"yolov12n-face.pt\")\n",
    "\n",
    "    # LSTM artifacts (pakai lstm_s v2)\n",
    "    LSTM_ARTIFACT_PKL: str = str(\n",
    "        BASE_DIR / \"model\" / \"trained\" / \"lstm_s\" / \"lstm_model_v1.pkl\"\n",
    "    )\n",
    "    LSTM_MODEL_DIR: str = str(BASE_DIR / \"model\" / \"trained\" / \"lstm_s\")\n",
    "\n",
    "    # Input video\n",
    "    VIDEO_PATH: str = str(\n",
    "        BASE_DIR\n",
    "        / \"assets\"\n",
    "        / \"dataset\"\n",
    "        / \"v2\"\n",
    "        / \"WhatsApp Video 2025-11-21 at 01.23.21.mp4\"\n",
    "    )\n",
    "\n",
    "    # Dataset wajah (folder berisi subfolder: rusdi, nawfal, rio, ...)\n",
    "    KNOWN_FACES_DIR: str = str(BASE_DIR / \"assets\" / \"dataset\" / \"face\")\n",
    "\n",
    "    # Layout / sequence\n",
    "    FIXED_SIZE: int = 640           # tinggi normalisasi untuk tampilan (optional)\n",
    "    SMALL_SIZE: Tuple[int, int] = (150, 150)\n",
    "    NUM_FRAMES: int = 15            # panjang sequence LSTM\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18ad5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_build_trt_engine(model_path: str, device: str = \"0\", half: bool = True):\n",
    "    \"\"\"\n",
    "    Load YOLO TensorRT engine jika sudah ada.\n",
    "    Jika masih .pt → convert otomatis menjadi .engine (task='detect').\n",
    "    \"\"\"\n",
    "    if model_path.endswith(\".engine\"):\n",
    "        print(f\"[TensorRT] Loading existing .engine: {model_path}\")\n",
    "        return YOLO(model_path)\n",
    "\n",
    "    if not model_path.endswith(\".pt\"):\n",
    "        raise ValueError(f\"Model path bukan .pt atau .engine: {model_path}\")\n",
    "\n",
    "    engine_path = model_path.replace(\".pt\", \".engine\")\n",
    "\n",
    "    if os.path.exists(engine_path):\n",
    "        print(f\"[TensorRT] Engine sudah ada, loading: {engine_path}\")\n",
    "        return YOLO(engine_path)\n",
    "\n",
    "    print(f\"[TensorRT] Converting {model_path} → {engine_path}\")\n",
    "    model = YOLO(model_path)\n",
    "    model.export(\n",
    "        format=\"engine\",\n",
    "        device=device,\n",
    "        half=half,\n",
    "        task=\"detect\",\n",
    "    )\n",
    "    print(f\"[TensorRT] DONE converting → {engine_path}\")\n",
    "    return YOLO(engine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d225b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_yolo_loader(path: str) -> Optional[YOLO]:\n",
    "    \"\"\"\n",
    "    Loader aman:\n",
    "    - Kalau file tidak ada → return None (tidak crash).\n",
    "    - Kalau .engine → YOLO(engine).\n",
    "    - Kalau .pt → load_or_build_trt_engine.\n",
    "    \"\"\"\n",
    "    if not path:\n",
    "        print(\"[YOLO] Path kosong, skip.\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[YOLO] File model tidak ditemukan, skip: {path}\")\n",
    "        return None\n",
    "\n",
    "    if path.endswith(\".engine\"):\n",
    "        print(f\"[YOLO] Loading engine: {path}\")\n",
    "        return YOLO(path)\n",
    "    elif path.endswith(\".pt\"):\n",
    "        print(f\"[YOLO] Loading/Converting PT: {path}\")\n",
    "        return load_or_build_trt_engine(path)\n",
    "    else:\n",
    "        print(f\"[YOLO] Format model tidak didukung: {path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f716b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_yolo_models(cfg: Config) -> Dict[str, Optional[YOLO]]:\n",
    "    return {\n",
    "        \"person\": smart_yolo_loader(cfg.PERSON_MODEL_PATH),\n",
    "        \"weapon\": smart_yolo_loader(cfg.WEAPON_MODEL_PATH),\n",
    "        \"face\": smart_yolo_loader(cfg.FACE_MODEL_PATH),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a461b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lstm_artifacts(cfg: Config):\n",
    "    \"\"\"\n",
    "    Load scaler, label_encoder, dan model LSTM Keras dari pkl.\n",
    "    Abaikan path lama di dalam pkl, pakai hanya nama file keras-nya.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(cfg.LSTM_ARTIFACT_PKL):\n",
    "        raise FileNotFoundError(\n",
    "            f\"LSTM artifact pkl tidak ditemukan: {cfg.LSTM_ARTIFACT_PKL}\"\n",
    "        )\n",
    "\n",
    "    with open(cfg.LSTM_ARTIFACT_PKL, \"rb\") as f:\n",
    "        artifacts = pickle.load(f)\n",
    "\n",
    "    scaler = artifacts[\"scaler\"]\n",
    "    label_encoder = artifacts[\"label_encoder\"]\n",
    "    keras_model_path = artifacts[\"model_filename\"]  # bisa berisi path lama\n",
    "    num_classes = artifacts.get(\"num_classes\", None)\n",
    "\n",
    "    keras_filename = os.path.basename(keras_model_path)\n",
    "    full_model_path = os.path.normpath(os.path.join(cfg.LSTM_MODEL_DIR, keras_filename))\n",
    "\n",
    "    print(f\"[LSTM] Will load model from: {full_model_path}\")\n",
    "\n",
    "    if not os.path.exists(full_model_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"File model LSTM Keras tidak ditemukan: {full_model_path}\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        model_pred = tf.keras.models.load_model(full_model_path)\n",
    "        print(f\"[LSTM] Model Keras loaded from: {full_model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[LSTM] Error loading Keras model: {e}\")\n",
    "        model_pred = None\n",
    "\n",
    "    return {\n",
    "        \"scaler\": scaler,\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"model_pred\": model_pred,\n",
    "        \"num_classes\": num_classes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e9f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_yolo(\n",
    "    frame: np.ndarray, face_model: Optional[YOLO], conf: float = 0.25\n",
    "):\n",
    "    \"\"\"\n",
    "    Deteksi wajah dengan YOLO-face, return list bbox:\n",
    "    (top, right, bottom, left, conf)  -> format yg cocok dengan face_recognition.\n",
    "    \"\"\"\n",
    "    if face_model is None:\n",
    "        return []\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    results = face_model(frame, conf=conf, verbose=False)\n",
    "    boxes_out = []\n",
    "\n",
    "    res0 = results[0]\n",
    "    if res0.boxes is None or len(res0.boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    for box in res0.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(w, x2), min(h, y2)\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "        conf_box = float(box.conf)\n",
    "        # face_recognition order: top, right, bottom, left\n",
    "        boxes_out.append((y1, x2, y2, x1, conf_box))\n",
    "\n",
    "    return boxes_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada8b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_face_database(\n",
    "    cfg: Config,\n",
    "    face_model: Optional[YOLO],\n",
    "    exts=(\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Scan assets/dataset/face/<nama>/... dan encode semua wajah.\n",
    "    - Deteksi wajah dulu pakai YOLO-face.\n",
    "    - Encode pakai face_recognition di bbox hasil YOLO.\n",
    "\n",
    "    Folder name = label (rusdi, nawfal, rio, dll).\n",
    "    Return dict dengan:\n",
    "      - encodings: np.ndarray (N, 128)\n",
    "      - names: List[str]\n",
    "    \"\"\"\n",
    "    base_dir = cfg.KNOWN_FACES_DIR\n",
    "    if not os.path.isdir(base_dir):\n",
    "        print(f\"[FaceDB] Folder tidak ditemukan: {base_dir}\")\n",
    "        return None\n",
    "\n",
    "    if face_model is None:\n",
    "        print(\"[FaceDB] YOLO face model tidak tersedia, tidak bisa build DB.\")\n",
    "        return None\n",
    "\n",
    "    all_encodings = []\n",
    "    all_names = []\n",
    "\n",
    "    print(f\"[FaceDB] Building face DB from: {base_dir}\")\n",
    "    for person_name in os.listdir(base_dir):\n",
    "        person_dir = os.path.join(base_dir, person_name)\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(person_dir):\n",
    "            fpath = os.path.join(person_dir, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                continue\n",
    "\n",
    "            # filter kasar by ekstensi\n",
    "            if \".\" in fname and not fname.endswith(exts):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                img_bgr = cv2.imread(fpath)\n",
    "                if img_bgr is None:\n",
    "                    print(f\"[FaceDB] Gagal baca gambar: {fpath}\")\n",
    "                    continue\n",
    "\n",
    "                # Deteksi wajah pakai YOLO-face\n",
    "                face_boxes = detect_faces_yolo(img_bgr, face_model, conf=0.4)\n",
    "                if not face_boxes:\n",
    "                    print(f\"[FaceDB] Tidak ada wajah terdeteksi (YOLO): {fpath}\")\n",
    "                    continue\n",
    "\n",
    "                img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "                boxes_fr = [(t, r, b, l) for (t, r, b, l, c) in face_boxes]\n",
    "\n",
    "                encs = face_recognition.face_encodings(\n",
    "                    img_rgb,\n",
    "                    known_face_locations=boxes_fr,\n",
    "                    model=\"large\",\n",
    "                )\n",
    "\n",
    "                if not encs:\n",
    "                    print(f\"[FaceDB] face_recognition tidak menemukan wajah: {fpath}\")\n",
    "                    continue\n",
    "\n",
    "                for enc in encs:\n",
    "                    all_encodings.append(enc)\n",
    "                    all_names.append(person_name)\n",
    "\n",
    "                print(f\"[FaceDB] Encoded {person_name}: {fname} (faces={len(encs)})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[FaceDB] Skip {fpath}: {e}\")\n",
    "\n",
    "    if not all_encodings:\n",
    "        print(\"[FaceDB] No face encodings collected.\")\n",
    "        return None\n",
    "\n",
    "    encodings_arr = np.array(all_encodings, dtype=np.float32)\n",
    "    print(f\"[FaceDB] Total encodings: {len(all_names)}\")\n",
    "    return {\"encodings\": encodings_arr, \"names\": all_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54756a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces_in_frame(\n",
    "    frame_bgr: np.ndarray,\n",
    "    face_boxes_tr: List[tuple],\n",
    "    face_db: Optional[dict],\n",
    "    match_threshold: float = 0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    - face_boxes_tr: list (top, right, bottom, left, conf) dari YOLO face\n",
    "    - face_db: {\"encodings\": np.ndarray(N,128), \"names\": List[str]}\n",
    "    Menggambar bbox + label nama langsung di frame_bgr.\n",
    "    \"\"\"\n",
    "    if face_db is None or not face_boxes_tr:\n",
    "        return frame_bgr\n",
    "\n",
    "    known_encodings = face_db[\"encodings\"]\n",
    "    known_names = face_db[\"names\"]\n",
    "\n",
    "    boxes_fr = [(t, r, b, l) for (t, r, b, l, c) in face_boxes_tr]\n",
    "    confs = [c for (_, _, _, _, c) in face_boxes_tr]\n",
    "\n",
    "    try:\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    except Exception:\n",
    "        frame_rgb = frame_bgr[:, :, ::-1]\n",
    "\n",
    "    encodings = face_recognition.face_encodings(\n",
    "        frame_rgb,\n",
    "        known_face_locations=boxes_fr,\n",
    "        model=\"large\",\n",
    "    )\n",
    "\n",
    "    for enc, (top, right, bottom, left), conf in zip(encodings, boxes_fr, confs):\n",
    "        if known_encodings.size == 0:\n",
    "            name = \"Unknown\"\n",
    "            dist_val = None\n",
    "        else:\n",
    "            enc_vec = enc.astype(np.float32)\n",
    "            dists = np.linalg.norm(known_encodings - enc_vec, axis=1)\n",
    "            idx_min = int(np.argmin(dists))\n",
    "            dist_val = float(dists[idx_min])\n",
    "            if dist_val < match_threshold:\n",
    "                name = known_names[idx_min]\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "\n",
    "        color = (0, 255, 0) if name != \"Unknown\" else (0, 0, 255)\n",
    "        label = (\n",
    "            f\"{name}\" if dist_val is None else f\"{name} ({dist_val:.2f}) ({conf:.2f})\"\n",
    "        )\n",
    "\n",
    "        cv2.rectangle(frame_bgr, (left, top), (right, bottom), color, 2)\n",
    "        cv2.putText(\n",
    "            frame_bgr,\n",
    "            label,\n",
    "            (left, top - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            color,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    return frame_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6230dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEAPON_CLASS_NAMES = {\n",
    "    0: \"knife\",\n",
    "    1: \"long_weapon\",\n",
    "    2: \"pistol\",\n",
    "}\n",
    "\n",
    "\n",
    "def detect_weapons_yolo(\n",
    "    frame: np.ndarray,\n",
    "    weapon_model: Optional[YOLO],\n",
    "    conf: float = 0.35,\n",
    "):\n",
    "    \"\"\"\n",
    "    Deteksi senjata dari frame original.\n",
    "    Return list: (x1, y1, x2, y2, conf, label)\n",
    "    \"\"\"\n",
    "    detections = []\n",
    "    if weapon_model is None:\n",
    "        return detections\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    results = weapon_model(frame, conf=conf, verbose=False)\n",
    "    res0 = results[0]\n",
    "\n",
    "    if res0.boxes is None or len(res0.boxes) == 0:\n",
    "        return detections\n",
    "\n",
    "    for box in res0.boxes:\n",
    "        cls_id = int(box.cls)\n",
    "        if cls_id not in WEAPON_CLASS_NAMES:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(w, x2), min(h, y2)\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "\n",
    "        conf_box = float(box.conf)\n",
    "        label = WEAPON_CLASS_NAMES[cls_id]\n",
    "        detections.append((x1, y1, x2, y2, conf_box, label))\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ba38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weapon_detections(frame: np.ndarray, detections):\n",
    "    \"\"\"\n",
    "    Gambar bbox & label senjata di atas frame (termasuk confidence).\n",
    "    \"\"\"\n",
    "    for x1, y1, x2, y2, conf, label in detections:\n",
    "        # warna beda untuk pistol & long weapon (bahaya) dan knife\n",
    "        if label in (\"pistol\", \"long_weapon\"):\n",
    "            color = (0, 0, 255)  # merah\n",
    "        else:\n",
    "            color = (0, 165, 255)  # oranye\n",
    "\n",
    "        # bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)\n",
    "\n",
    "        # text dengan conf\n",
    "        text = f\"{label.upper()}  {conf:.2f}\"\n",
    "\n",
    "        # background agar tulisan selalu terbaca\n",
    "        (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        cv2.rectangle(frame, (x1, y1 - th - 8), (x1 + tw + 8, y1), color, -1)\n",
    "\n",
    "        # tulis text\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            text,\n",
    "            (x1 + 4, y1 - 5),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feb2bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_person_yolo(\n",
    "    frame: np.ndarray, person_model: Optional[YOLO], conf: float = 0.35\n",
    "):\n",
    "    \"\"\"\n",
    "    Deteksi manusia di full frame (class 0).\n",
    "    Return list: (x1, y1, x2, y2, conf)\n",
    "    \"\"\"\n",
    "    if person_model is None:\n",
    "        return []\n",
    "\n",
    "    results = person_model(frame, classes=[0], conf=conf, verbose=False)\n",
    "    dets = []\n",
    "    res0 = results[0]\n",
    "\n",
    "    if res0.boxes is None or len(res0.boxes) == 0:\n",
    "        return dets\n",
    "\n",
    "    for box in res0.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "        conf_box = float(box.conf)\n",
    "        dets.append((x1, y1, x2, y2, conf_box))\n",
    "\n",
    "    return dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4687be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_person_detections(frame: np.ndarray, detections):\n",
    "    \"\"\"\n",
    "    Gambar bbox & label 'PERSON' di atas frame.\n",
    "    \"\"\"\n",
    "    for x1, y1, x2, y2, conf in detections:\n",
    "        color = (0, 150, 255)  # biru-oranye\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        text = f\"PERSON {conf:.2f}\"\n",
    "        (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        cv2.rectangle(frame, (x1, y1 - th - 8), (x1 + tw + 8, y1), color, -1)\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            text,\n",
    "            (x1 + 4, y1 - 5),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3e2b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_person_for_motion(person_dets):\n",
    "    \"\"\"\n",
    "    Pilih 1 person bbox yang paling cocok untuk dianalisis motion.\n",
    "    Pilih bounding box dengan luas terbesar.\n",
    "    person_dets format: (x1, y1, x2, y2, conf)\n",
    "    \"\"\"\n",
    "    if not person_dets:\n",
    "        return None\n",
    "\n",
    "    max_area = 0\n",
    "    best = None\n",
    "    for x1, y1, x2, y2, conf in person_dets:\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            best = (x1, y1, x2, y2, conf)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bad87ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_to_canvas(img: np.ndarray, fixed_size: int):\n",
    "    \"\"\"\n",
    "    Resize img ke dalam canvas fixed_size x fixed_size dengan padding putih.\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    scale = fixed_size / max(w, h)\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    canvas = np.full((fixed_size, fixed_size, 3), 255, dtype=np.uint8)\n",
    "    dw = fixed_size - new_w\n",
    "    dh = fixed_size - new_h\n",
    "    top = dh // 2\n",
    "    left = dw // 2\n",
    "    canvas[top : top + new_h, left : left + new_w] = resized\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cf5fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_state(state: int, lndmrkX: Dict[str, float]) -> int:\n",
    "    \"\"\"\n",
    "    Menentukan arah wajah (kiri/tengah/kanan) berdasarkan posisi X hidung\n",
    "    relatif terhadap landmark lain.\n",
    "    state = 0 atau 1 untuk membalik interpretasi arah jika perlu.\n",
    "    \"\"\"\n",
    "    nose_x = lndmrkX[\"nose\"]\n",
    "    min_x = min(lndmrkX.values())\n",
    "    max_x = max(lndmrkX.values())\n",
    "\n",
    "    if state == 0:\n",
    "        if nose_x == min_x:\n",
    "            direction = 1  # Kiri\n",
    "        elif nose_x == max_x:\n",
    "            direction = 2  # Kanan\n",
    "        else:\n",
    "            direction = 0  # Tengah\n",
    "    else:\n",
    "        if nose_x == min_x:\n",
    "            direction = 2  # Kanan (dibalik)\n",
    "        elif nose_x == max_x:\n",
    "            direction = 1  # Kiri\n",
    "        else:\n",
    "            direction = 0  # Tengah\n",
    "\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ffe7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_state(state: int, lndmrkZ: Dict[str, float]) -> int:\n",
    "    \"\"\"\n",
    "    Menentukan apakah tangan 'terlihat' berdasarkan nilai Z (kedalaman).\n",
    "    \"\"\"\n",
    "    nose_z = lndmrkZ[\"nose\"]\n",
    "    wrist_r_z = lndmrkZ[\"wrist_r\"]\n",
    "    wrist_l_z = lndmrkZ[\"wrist_l\"]\n",
    "\n",
    "    visible = wrist_r_z < nose_z and wrist_l_z < nose_z\n",
    "\n",
    "    if state == 0:\n",
    "        hand = 1 if visible else 0\n",
    "    else:\n",
    "        hand = 0 if visible else 1\n",
    "\n",
    "    return hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e7915ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ext_feature_value(lndmrk) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    lndmrk: dict dengan key 'nose', 'ear_l', 'ear_r', 'wrist_r', 'wrist_l'\n",
    "    value = NormalizedLandmark\n",
    "    Return: (side, state, hand)\n",
    "    \"\"\"\n",
    "    nose = lndmrk[\"nose\"]\n",
    "    ear_l = lndmrk[\"ear_l\"]\n",
    "    ear_r = lndmrk[\"ear_r\"]\n",
    "    wrist_r = lndmrk[\"wrist_r\"]\n",
    "    wrist_l = lndmrk[\"wrist_l\"]\n",
    "\n",
    "    noseX, noseY, noseZ = nose.x, nose.y, nose.z\n",
    "    earLX, earLY, earLZ = ear_l.x, ear_l.y, ear_l.z\n",
    "    earRX, earRY, earRZ = ear_r.x, ear_r.y, ear_r.z\n",
    "    wristRX, wristRY, wristRZ = wrist_r.x, wrist_r.y, wrist_r.z\n",
    "    wristLX, wristLY, wristLZ = wrist_l.x, wrist_l.y, wrist_l.z\n",
    "\n",
    "    lndmrkX = {\n",
    "        \"nose\": noseX,\n",
    "        \"ear_l\": earLX,\n",
    "        \"ear_r\": earRX,\n",
    "        \"wrist_r\": wristRX,\n",
    "        \"wrist_l\": wristLX,\n",
    "    }\n",
    "    lndmrkZ = {\n",
    "        \"nose\": noseZ,\n",
    "        \"ear_l\": earLZ,\n",
    "        \"ear_r\": earRZ,\n",
    "        \"wrist_r\": wristRZ,\n",
    "        \"wrist_l\": wristLZ,\n",
    "    }\n",
    "\n",
    "    if noseZ < min(earLZ, earRZ):\n",
    "        state = 0\n",
    "    else:\n",
    "        state = 1\n",
    "\n",
    "    side = side_state(state, lndmrkX)\n",
    "    hand = hand_state(state, lndmrkZ)\n",
    "\n",
    "    return side, state, hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a764732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionSequenceBuffer:\n",
    "    \"\"\"\n",
    "    Menyimpan NUM_FRAMES terakhir:\n",
    "    - frames kecil (thumbnail),\n",
    "    - landmark hasil deteksi,\n",
    "    - fitur tambahan (face_direction, face_shown, hand_shown).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_frames: int):\n",
    "        self.num_frames = num_frames\n",
    "        self.frames: List[np.ndarray] = []\n",
    "        self.detected: List[List[landmark_pb2.NormalizedLandmark]] = []\n",
    "        self.face_direction: List[int] = []\n",
    "        self.face_shown: List[int] = []\n",
    "        self.hand_shown: List[int] = []\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        frame_small: np.ndarray,\n",
    "        landmarks: List[landmark_pb2.NormalizedLandmark],\n",
    "        ext_feature: Tuple[int, int, int],\n",
    "    ):\n",
    "        side, state, hand = ext_feature\n",
    "\n",
    "        if len(self.frames) >= self.num_frames:\n",
    "            self.frames.pop(0)\n",
    "            self.detected.pop(0)\n",
    "            self.face_direction.pop(0)\n",
    "            self.face_shown.pop(0)\n",
    "            self.hand_shown.pop(0)\n",
    "\n",
    "        self.frames.append(frame_small)\n",
    "        self.detected.append(landmarks)\n",
    "        self.face_direction.append(side)\n",
    "        self.face_shown.append(state)\n",
    "        self.hand_shown.append(hand)\n",
    "\n",
    "    def is_full(self) -> bool:\n",
    "        return len(self.frames) == self.num_frames\n",
    "\n",
    "    def build_landmark_list(self) -> Optional[landmark_pb2.NormalizedLandmarkList]:\n",
    "        if not self.detected:\n",
    "            return None\n",
    "        flat_detected = [lm for sublist in self.detected for lm in sublist]\n",
    "        landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "        landmark_list.landmark.extend(flat_detected)\n",
    "        return landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba411228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layout(\n",
    "    buffer: MotionSequenceBuffer,\n",
    "    base_frame: np.ndarray,\n",
    "    cfg: Config,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Layout final:\n",
    "    - base_frame (frame original + overlay)\n",
    "    - 5 kolom history frame (jika buffer penuh 15 frame)\n",
    "    \"\"\"\n",
    "    if not buffer.is_full():\n",
    "        return base_frame\n",
    "\n",
    "    SMALL_W, _ = cfg.SMALL_SIZE\n",
    "    stacked_columns = []\n",
    "    for col_idx in range(5):\n",
    "        start = col_idx * 3\n",
    "        end = start + 3\n",
    "        column_frames = buffer.frames[start:end]\n",
    "        stacked = np.vstack(column_frames)\n",
    "        stacked = cv2.resize(stacked, (SMALL_W, cfg.FIXED_SIZE))\n",
    "        stacked_columns.append(stacked)\n",
    "\n",
    "    final_layout = np.hstack([base_frame] + stacked_columns)\n",
    "    return final_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "000cbbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_motion_feature_vector(\n",
    "    buffer: MotionSequenceBuffer,\n",
    "    landmark_list: landmark_pb2.NormalizedLandmarkList,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Struktur:\n",
    "    - Untuk setiap landmark: x, y, z, visibility\n",
    "    - Setiap 5 landmark (1 frame): tambah 3 fitur (side, state, hand)\n",
    "    \"\"\"\n",
    "    motion_row = []\n",
    "    counter = 0\n",
    "    feature_idx = 0\n",
    "\n",
    "    for lndmrk in landmark_list.landmark:\n",
    "        motion_row.extend([lndmrk.x, lndmrk.y, lndmrk.z, lndmrk.visibility])\n",
    "        counter += 1\n",
    "\n",
    "        if counter % 5 == 0 and feature_idx < len(buffer.face_direction):\n",
    "            motion_row.append(buffer.face_direction[feature_idx])\n",
    "            motion_row.append(buffer.face_shown[feature_idx])\n",
    "            motion_row.append(buffer.hand_shown[feature_idx])\n",
    "            feature_idx += 1\n",
    "\n",
    "    return motion_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9b39d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_motion_class(\n",
    "    motion_row: List[float],\n",
    "    scaler,\n",
    "    model_pred,\n",
    "    label_encoder,\n",
    "    min_conf: float = 0.0,\n",
    ") -> Tuple[Optional[str], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Normalisasi fitur, reshape, dan prediksi kelas dengan LSTM.\n",
    "\n",
    "    Return:\n",
    "      - motion_class: str atau None\n",
    "      - conf: float (prob max) atau None\n",
    "\n",
    "    min_conf:\n",
    "      - kalau prob max < min_conf → return (None, None)\n",
    "    \"\"\"\n",
    "    if model_pred is None:\n",
    "        return None, None\n",
    "\n",
    "    X_data = np.array(motion_row).reshape(1, -1)\n",
    "    X_scaled = scaler.transform(X_data)\n",
    "    X_reshaped = X_scaled.reshape(1, 1, X_scaled.shape[1])\n",
    "\n",
    "    try:\n",
    "        y_pred_probs = model_pred.predict(\n",
    "            X_reshaped, verbose=0\n",
    "        )  # shape: (1, num_classes)\n",
    "        probs = y_pred_probs[0]\n",
    "        y_pred_index = int(np.argmax(probs))\n",
    "        conf = float(probs[y_pred_index])\n",
    "\n",
    "        if conf < min_conf:\n",
    "            return None, None\n",
    "\n",
    "        motion_class = label_encoder.inverse_transform([y_pred_index])[0]\n",
    "        return motion_class, conf\n",
    "    except Exception as e:\n",
    "        print(f\"[Predict] Error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54154fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YOLO] Loading/Converting PT: /mnt/d/Programming/PycharmProjects/suspicious_detection/model/yolo/yolo12n.pt\n",
      "[TensorRT] Engine sudah ada, loading: /mnt/d/Programming/PycharmProjects/suspicious_detection/model/yolo/yolo12n.engine\n",
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "[YOLO] Loading/Converting PT: /mnt/d/Programming/PycharmProjects/suspicious_detection/model/yolo/yolov12n-weapon.pt\n",
      "[TensorRT] Engine sudah ada, loading: /mnt/d/Programming/PycharmProjects/suspicious_detection/model/yolo/yolov12n-weapon.engine\n",
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "[YOLO] Loading/Converting PT: /mnt/d/Programming/PycharmProjects/suspicious_detection/model/yolo/yolov12n-face.pt\n",
      "[TensorRT] Engine sudah ada, loading: /mnt/d/Programming/PycharmProjects/suspicious_detection/model/yolo/yolov12n-face.engine\n",
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "[LSTM] Will load model from: /mnt/d/Programming/PycharmProjects/suspicious_detection/model/trained/lstm_s/lstm_weights_v1.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruvne/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ruvne/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.7.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "I0000 00:00:1763661519.781775  319762 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5561 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Model Keras loaded from: /mnt/d/Programming/PycharmProjects/suspicious_detection/model/trained/lstm_s/lstm_weights_v1.keras\n",
      "[FaceDB] Building face DB from: /mnt/d/Programming/PycharmProjects/suspicious_detection/assets/dataset/face\n",
      "Loading /mnt/d/Programming/PycharmProjects/suspicious_detection/model/yolo/yolov12n-face.engine for TensorRT inference...\n",
      "[11/21/2025-01:58:40] [TRT] [I] Loaded engine size: 10 MiB\n",
      "[11/21/2025-01:58:40] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +13, now: CPU 1, GPU 18 (MiB)\n",
      "[FaceDB] Encoded nawfal: 9800a5c1-4aa4-4118-82ed-c53a62763ad7.jpg (faces=1)\n",
      "[FaceDB] Encoded rio: 73ae6ec6-85ed-4e0c-82b3-ad13dfacf6f8.jpg (faces=1)\n",
      "[FaceDB] Encoded rusdi: 11211005_Foto_Ahmad Rusdianto Andarina Syakbani Square.jpg (faces=1)\n",
      "[FaceDB] Total encodings: 3\n",
      "Loading /mnt/d/Programming/PycharmProjects/suspicious_detection/model/yolo/yolov12n-weapon.engine for TensorRT inference...\n",
      "[11/21/2025-01:58:42] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[11/21/2025-01:58:42] [TRT] [I] Loaded engine size: 10 MiB\n",
      "[11/21/2025-01:58:42] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +12, now: CPU 2, GPU 35 (MiB)\n",
      "Loading /mnt/d/Programming/PycharmProjects/suspicious_detection/model/yolo/yolo12n.engine for TensorRT inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763661522.652218  319762 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1763661522.674707  320005 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: D3D12 (NVIDIA GeForce RTX 4060 Laptop GPU)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1763661522.748120  319968 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763661522.785291  319980 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763661522.791854  319971 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763661522.791992  319975 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763661522.793636  319972 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763661522.807739  319987 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763661522.813880  319994 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763661522.815577  319979 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/21/2025-01:58:42] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[11/21/2025-01:58:42] [TRT] [I] Loaded engine size: 10 MiB\n",
      "[11/21/2025-01:58:42] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +13, now: CPU 3, GPU 53 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1763661523.125280  319990 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763661524.461780  319911 service.cc:152] XLA service 0x7f3648003770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1763661524.461804  319911 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-11-21 01:58:44.470548: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1763661524.509823  319911 cuda_dnn.cc:529] Loaded cuDNN version 90600\n",
      "I0000 00:00:1763661524.647592  319911 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "def process_video(cfg: Config):\n",
    "    # === Load semua model ===\n",
    "    yolo_models = load_all_yolo_models(cfg)\n",
    "    lstm_artifacts = load_lstm_artifacts(cfg)\n",
    "    scaler = lstm_artifacts[\"scaler\"]\n",
    "    label_encoder = lstm_artifacts[\"label_encoder\"]\n",
    "    model_pred = lstm_artifacts[\"model_pred\"]\n",
    "\n",
    "    # Face DB (rusdi / nawfal / rio) pakai YOLO-face\n",
    "    face_db = load_face_database(cfg, yolo_models[\"face\"])\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "    cap = cv2.VideoCapture(cfg.VIDEO_PATH)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {cfg.VIDEO_PATH}\")\n",
    "        return\n",
    "\n",
    "    buffer = MotionSequenceBuffer(cfg.NUM_FRAMES)\n",
    "\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    "    ) as holistic:\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # === 1) FULL FRAME UNTUK DISPLAY ===\n",
    "            frame_disp = frame.copy()\n",
    "            h, w = frame_disp.shape[:2]\n",
    "            if h != cfg.FIXED_SIZE:\n",
    "                scale = cfg.FIXED_SIZE / float(h)\n",
    "                new_w = int(w * scale)\n",
    "                frame_disp = cv2.resize(frame_disp, (new_w, cfg.FIXED_SIZE))\n",
    "\n",
    "            # === 2) DETEKSI SENJATA DI FULL FRAME ===\n",
    "            weapon_dets = detect_weapons_yolo(\n",
    "                frame_disp,\n",
    "                yolo_models[\"weapon\"],\n",
    "                conf=0.5,\n",
    "            )\n",
    "            draw_weapon_detections(frame_disp, weapon_dets)\n",
    "\n",
    "            # === 2.5) DETEKSI PERSON DI FULL FRAME (sekali saja) ===\n",
    "            person_dets = detect_person_yolo(\n",
    "                frame_disp,\n",
    "                yolo_models[\"person\"],\n",
    "                conf=0.35,\n",
    "            )\n",
    "            draw_person_detections(frame_disp, person_dets)\n",
    "\n",
    "            # pilih 1 person untuk motion (bbox terbesar)\n",
    "            best_person = select_person_for_motion(person_dets)\n",
    "\n",
    "            # === 3) (OPSIONAL) FACE RECOG DI FULL FRAME ===\n",
    "            face_boxes = detect_faces_yolo(frame_disp, yolo_models[\"face\"], conf=0.4)\n",
    "            if face_boxes and face_db is not None:\n",
    "                recognize_faces_in_frame(\n",
    "                    frame_disp, face_boxes, face_db, match_threshold=0.3\n",
    "                )\n",
    "\n",
    "            # === 4) CROP PERSON (HANYA UNTUK MOTION MODEL) ===\n",
    "            person_crop = None\n",
    "            if best_person is not None:\n",
    "                x1, y1, x2, y2, _ = best_person\n",
    "                person_raw = frame_disp[y1:y2, x1:x2]\n",
    "                if person_raw.size > 0:\n",
    "                    person_crop = fit_to_canvas(person_raw, cfg.FIXED_SIZE)\n",
    "\n",
    "            if person_crop is None:\n",
    "                # Tidak ada orang terdeteksi → tetap tampilkan full frame dengan weapon/face\n",
    "                final_layout = frame_disp\n",
    "                cv2.imshow(\"Video\", final_layout)\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # === 5) MOTION PIPELINE DI person_crop ===\n",
    "            frame_small = cv2.resize(person_crop, cfg.SMALL_SIZE)\n",
    "\n",
    "            # MediaPipe Holistic di crop orang\n",
    "            crop_rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n",
    "            crop_rgb.flags.writeable = False\n",
    "            results = holistic.process(crop_rgb)\n",
    "            crop_rgb.flags.writeable = True\n",
    "\n",
    "            detection_successful = False\n",
    "            current_landmarks: List[landmark_pb2.NormalizedLandmark] = []\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                pose_lm = results.pose_landmarks.landmark\n",
    "                nose = pose_lm[mp_holistic.PoseLandmark.NOSE]\n",
    "                ear_r = pose_lm[mp_holistic.PoseLandmark.RIGHT_EAR]\n",
    "                ear_l = pose_lm[mp_holistic.PoseLandmark.LEFT_EAR]\n",
    "                wrist_r = pose_lm[mp_holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                wrist_l = pose_lm[mp_holistic.PoseLandmark.LEFT_WRIST]\n",
    "\n",
    "                current_landmarks.extend([nose, ear_l, ear_r, wrist_r, wrist_l])\n",
    "                detection_successful = True\n",
    "\n",
    "            if not detection_successful:\n",
    "                # Pose gagal tapi weapon/face tetap jalan\n",
    "                final_layout = frame_disp\n",
    "                cv2.imshow(\"Video\", final_layout)\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # === 6) FITUR TAMBAHAN (side/state/hand) DARI LANDMARK DI person_crop ===\n",
    "            ext_feature = get_ext_feature_value(\n",
    "                {\n",
    "                    \"nose\": nose,\n",
    "                    \"ear_r\": ear_r,\n",
    "                    \"ear_l\": ear_l,\n",
    "                    \"wrist_r\": wrist_r,\n",
    "                    \"wrist_l\": wrist_l,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # === 7) UPDATE BUFFER SEQUENCE UNTUK LSTM ===\n",
    "            buffer.add(frame_small, current_landmarks, ext_feature)\n",
    "\n",
    "            # Layout: full frame + history crop person\n",
    "            final_layout = build_layout(buffer, frame_disp, cfg)\n",
    "\n",
    "            # === 8) PREDIKSI MOTION JIKA BUFFER PENUH ===\n",
    "            if buffer.is_full():\n",
    "                landmark_list = buffer.build_landmark_list()\n",
    "                if landmark_list is not None:\n",
    "                    motion_row = build_motion_feature_vector(buffer, landmark_list)\n",
    "                    motion_class, motion_conf = predict_motion_class(\n",
    "                        motion_row,\n",
    "                        scaler,\n",
    "                        model_pred,\n",
    "                        label_encoder,\n",
    "                        min_conf=0.5,  # boleh diubah\n",
    "                    )\n",
    "\n",
    "                    if motion_class is not None:\n",
    "                        cv2.putText(\n",
    "                            final_layout,\n",
    "                            f\"Class: {motion_class} ({motion_conf:.2f})\",\n",
    "                            (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1,\n",
    "                            (0, 0, 255),\n",
    "                            2,\n",
    "                        )\n",
    "\n",
    "            # === 9) TAMPILKAN ===\n",
    "            cv2.imshow(\"Video\", final_layout)\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_video(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
