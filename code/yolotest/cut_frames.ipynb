{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1aea01",
   "metadata": {},
   "source": [
    "# Create rectangle image focus to object.\n",
    "From an full image of cctv, detect someone then create\n",
    "rectangle image that shows rectangle. \n",
    "\n",
    "\n",
    "Target : The result image used to detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f46f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5938aa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/code/yolotest/../../assets/dump/woman.jpg: 320x640 1 person, 166.5ms\n",
      "Speed: 1.9ms preprocess, 166.5ms inference, 7.8ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"../../model/yolo/yolo12n.pt\")\n",
    "\n",
    "results = model(\"../../assets/dump/woman.jpg\")\n",
    "img = results[0].plot()  # draws boxes on the image\n",
    "cv2.imshow(\"Detection\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6307db9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/code/yolotest/../../assets/dump/people.jpg: 448x640 2 persons, 49.1ms\n",
      "Speed: 2.1ms preprocess, 49.1ms inference, 4.9ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO # Make sure this is installed (e.g., pip install ultralytics)\n",
    "import numpy as np # Import NumPy for array manipulation\n",
    "\n",
    "# Assuming this path and model are correct for your setup\n",
    "model = YOLO(\"../../model/yolo/yolo12n.pt\") \n",
    "# Assuming this path is correct, load the image with cv2 to manipulate it\n",
    "original_img = cv2.imread(\"../../assets/dump/sit.jpg\")\n",
    "\n",
    "# Perform the detection\n",
    "results = model(\"../../assets/dump/people.jpg\")\n",
    "\n",
    "# --- Extract Bounding Box and Crop ---\n",
    "\n",
    "# The 'results' object is a list of Results objects (one per image).\n",
    "# We take the first result, as there's only one image.\n",
    "boxes = results[0].boxes\n",
    "\n",
    "# Check if any objects were detected\n",
    "if len(boxes) > 0:\n",
    "    # If there are multiple detections, you'll likely want to iterate through them.\n",
    "    # For simplicity, let's crop to the *first* detected object.\n",
    "    \n",
    "    # Get the bounding box coordinates (xyxy format: [x_min, y_min, x_max, y_max])\n",
    "    # The 'xyxy' tensor needs to be converted to a NumPy array and then to integers.\n",
    "    # We use .cpu().numpy() to ensure it's on the CPU and in a numpy format.\n",
    "    x1, y1, x2, y2 = boxes.xyxy[0].cpu().numpy().astype(int)\n",
    "\n",
    "    # --- Crop the image ---\n",
    "    # NumPy array slicing: img[y_start:y_end, x_start:x_end]\n",
    "    cropped_img = original_img[y1:y2, x1:x2]\n",
    "    \n",
    "    # --- Display the cropped image ---\n",
    "    cv2.imshow(\"Cropped Detection\", cropped_img)\n",
    "\n",
    "else:\n",
    "    print(\"No objects detected in the image.\")\n",
    "\n",
    "# This waits indefinitely for a keypress and then closes all windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fe7d70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /mnt/c/users/nawfal/documents/apps/Collaborative_Project/suspicious_detection/code/yolotest/../../assets/dump/woman.jpg: 320x640 1 person, 19.3ms\n",
      "Speed: 1.9ms preprocess, 19.3ms inference, 6.4ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "FIXED_SIZE = 640  # Target width and height for the output frame\n",
    "\n",
    "# Assuming this path and model are correct for your setup\n",
    "model = YOLO(\"../../model/yolo/yolo12n.pt\") \n",
    "original_img = cv2.imread(\"../../assets/dump/woman.jpg\")\n",
    "\n",
    "# Perform the detection (runs on the original path, not the loaded cv2 object)\n",
    "results = model(\"../../assets/dump/woman.jpg\")\n",
    "\n",
    "# --- Extract Bounding Box and Process ---\n",
    "\n",
    "boxes = results[0].boxes\n",
    "\n",
    "# Check if any objects were detected\n",
    "if len(boxes) > 0:\n",
    "    # Get the bounding box coordinates for the first detected object\n",
    "    x1, y1, x2, y2 = boxes.xyxy[0].cpu().numpy().astype(int)\n",
    "\n",
    "    # 1. Crop the image based on the detected box\n",
    "    cropped_img = original_img[y1:y2, x1:x2]\n",
    "    \n",
    "    crop_h, crop_w = cropped_img.shape[:2]\n",
    "\n",
    "    # 2. Determine the scaling factor\n",
    "    # We want to fit the largest dimension (width or height) to the FIXED_SIZE\n",
    "    scale = FIXED_SIZE / max(crop_w, crop_h)\n",
    "    \n",
    "    # Calculate the new dimensions after scaling\n",
    "    new_w = int(crop_w * scale)\n",
    "    new_h = int(crop_h * scale)\n",
    "\n",
    "    # 3. Resize the cropped image\n",
    "    resized_img = cv2.resize(cropped_img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 4. Create the fixed-size white canvas (Padding/Letterboxing)\n",
    "    # A white image is represented by (255, 255, 255) for BGR\n",
    "    # The canvas must have 3 color channels (depth=3)\n",
    "    final_frame = np.full((FIXED_SIZE, FIXED_SIZE, 3), 255, dtype=np.uint8)\n",
    "\n",
    "    # 5. Calculate padding offsets to center the resized image\n",
    "    # dw and dh are the space left over after placing the image\n",
    "    dw = FIXED_SIZE - new_w\n",
    "    dh = FIXED_SIZE - new_h\n",
    "    \n",
    "    # Calculate the starting position (top-left corner) for centering\n",
    "    top = dh // 2\n",
    "    bottom = top + new_h\n",
    "    left = dw // 2\n",
    "    right = left + new_w\n",
    "\n",
    "    # 6. Place the resized image onto the white canvas\n",
    "    final_frame[top:bottom, left:right] = resized_img\n",
    "    \n",
    "    # --- Display the padded, fixed-size image ---\n",
    "    cv2.imshow(f\"Padded Detection ({FIXED_SIZE}x{FIXED_SIZE})\", final_frame)\n",
    "\n",
    "else:\n",
    "    print(\"No objects detected in the image.\")\n",
    "\n",
    "# This waits indefinitely for a keypress and then closes all windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d06e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
